{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282187e2-4061-4a8a-94c7-d592e1598a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled)/sum(y_train_resampled))\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba24a53-f2ae-4c6d-816c-0ded554a1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled)/sum(y_train_resampled) * 1.5)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_xgb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = best_xgb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Adjusted Threshold:\", threshold)\n",
    "print(\"Adjusted Accuracy:\", accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4b72d-acc8-4e8b-8c81-2d5c3434ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled)/sum(y_train_resampled) * 1.5)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Create an ensemble of models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', rf_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", accuracy)\n",
    "print(\"Ensemble Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e698d83-9f27-4e68-9b52-8831c99b4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled)/sum(y_train_resampled) * 1.5)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Create an ensemble of models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', rf_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", accuracy)\n",
    "print(\"Ensemble Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03220e1d-6b36-4b18-b641-c33e865d7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled)/sum(y_train_resampled) * 1.5)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Create an ensemble of models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', rf_classifier),\n",
    "    ('gb', gb_classifier),\n",
    "    ('lr', lr_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", accuracy)\n",
    "print(\"Ensemble Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "adjusted_classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Adjusted Threshold:\", threshold)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", adjusted_classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b26965-d202-4250-9a73-d964e30b5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled)/sum(y_train_resampled) * 1.5)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression classifier with increased max_iter\n",
    "lr_classifier = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Create an ensemble of models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', rf_classifier),\n",
    "    ('gb', gb_classifier),\n",
    "    ('lr', lr_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", accuracy)\n",
    "print(\"Ensemble Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "adjusted_classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Adjusted Threshold:\", threshold)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", adjusted_classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5757971-03a8-4acf-a782-629b07d87c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "df['num_medications_time_in_hospital'] = df['num_medications'] * df['time_in_hospital']\n",
    "df['num_procedures_time_in_hospital'] = df['num_procedures'] * df['time_in_hospital']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled)/sum(y_train_resampled) * 1.5)\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Initialize GridSearchCV for each classifier\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid_xgb, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_rf = GridSearchCV(estimator=rf_classifier, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_gb = GridSearchCV(estimator=gb_classifier, param_grid=param_grid_gb, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimators\n",
    "best_xgb_classifier = grid_search_xgb.best_estimator_\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "best_gb_classifier = grid_search_gb.best_estimator_\n",
    "\n",
    "# Create an ensemble of models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', best_rf_classifier),\n",
    "    ('gb', best_gb_classifier),\n",
    "    ('lr', lr_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", accuracy)\n",
    "print(\"Ensemble Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "adjusted_classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Adjusted Threshold:\", threshold)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", adjusted_classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee227269-e912-4ce7-b599-5298671cd523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "df['num_medications_time_in_hospital'] = df['num_medications'] * df['time_in_hospital']\n",
    "df['num_procedures_time_in_hospital'] = df['num_procedures'] * df['time_in_hospital']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled)/sum(y_train_resampled) * 1.5)\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Initialize GridSearchCV for each classifier\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid_xgb, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_rf = GridSearchCV(estimator=rf_classifier, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_gb = GridSearchCV(estimator=gb_classifier, param_grid=param_grid_gb, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimators\n",
    "best_xgb_classifier = grid_search_xgb.best_estimator_\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "best_gb_classifier = grid_search_gb.best_estimator_\n",
    "\n",
    "# Create an ensemble of models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', best_rf_classifier),\n",
    "    ('gb', best_gb_classifier),\n",
    "    ('lr', lr_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", accuracy)\n",
    "print(\"Ensemble Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "adjusted_classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Adjusted Threshold:\", threshold)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", adjusted_classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b45d6-0ed1-44d1-a821-d55cc04aba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (Assume preprocessing steps and feature engineering are already done as before)\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Continue with preprocessing...\n",
    "\n",
    "# Balance the dataset using SMOTE-Tomek\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Cost-sensitive XGBoost\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled) / sum(y_train_resampled))\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid_xgb, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb_classifier = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Initialize other classifiers with cost-sensitive learning\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Create an ensemble of models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', rf_classifier),\n",
    "    ('gb', gb_classifier),\n",
    "    ('lr', lr_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", accuracy)\n",
    "print(\"Ensemble Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "adjusted_classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Adjusted Threshold:\", threshold)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", adjusted_classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d26e6-cad9-470d-8e67-4bee041e825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "df['num_medications_time_in_hospital'] = df['num_medications'] * df['time_in_hospital']\n",
    "df['num_procedures_time_in_hospital'] = df['num_procedures'] * df['time_in_hospital']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled) / sum(y_train_resampled))\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Initialize GridSearchCV for each classifier\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid_xgb, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_rf = GridSearchCV(estimator=rf_classifier, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_gb = GridSearchCV(estimator=gb_classifier, param_grid=param_grid_gb, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimators\n",
    "best_xgb_classifier = grid_search_xgb.best_estimator_\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "best_gb_classifier = grid_search_gb.best_estimator_\n",
    "\n",
    "# Define the base learners\n",
    "base_learners = [\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', best_rf_classifier),\n",
    "    ('gb', best_gb_classifier)\n",
    "]\n",
    "\n",
    "# Define the meta-learner\n",
    "meta_learner = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Create the stacking ensemble\n",
    "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, cv=3)\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Stacking Model Accuracy:\", accuracy)\n",
    "print(\"Stacking Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = stacking_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "adjusted_classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Adjusted Threshold:\", threshold)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", adjusted_classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257dc486-80eb-4ca7-a2ff-ec097f53cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "df['num_medications_time_in_hospital'] = df['num_medications'] * df['time_in_hospital']\n",
    "df['num_procedures_time_in_hospital'] = df['num_procedures'] * df['time_in_hospital']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled) / sum(y_train_resampled))\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "mlp_classifier = MLPClassifier(random_state=42, max_iter=1000)\n",
    "svm_classifier = SVC(probability=True, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Initialize GridSearchCV for each classifier\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid_xgb, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_rf = GridSearchCV(estimator=rf_classifier, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_gb = GridSearchCV(estimator=gb_classifier, param_grid=param_grid_gb, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimators\n",
    "best_xgb_classifier = grid_search_xgb.best_estimator_\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "best_gb_classifier = grid_search_gb.best_estimator_\n",
    "\n",
    "# Define the base learners\n",
    "base_learners = [\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', best_rf_classifier),\n",
    "    ('gb', best_gb_classifier),\n",
    "    ('mlp', mlp_classifier),\n",
    "    ('svm', svm_classifier)\n",
    "]\n",
    "\n",
    "# Define the meta-learner\n",
    "meta_learner = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Create the stacking ensemble\n",
    "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, cv=3)\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Stacking Model Accuracy:\", accuracy)\n",
    "print(\"Stacking Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = stacking_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "adjusted_classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Adjusted Threshold:\", threshold)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", adjusted_classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe241b5-94d8-4b60-a7cf-bc203d3499da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "df['num_medications_time_in_hospital'] = df['num_medications'] * df['time_in_hospital']\n",
    "df['num_procedures_time_in_hospital'] = df['num_procedures'] * df['time_in_hospital']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist_xgb = {\n",
    "    'max_depth': sp_randint(3, 10),\n",
    "    'learning_rate': sp_uniform(0.01, 0.3),\n",
    "    'n_estimators': sp_randint(50, 300),\n",
    "    'subsample': sp_uniform(0.5, 0.5),\n",
    "    'colsample_bytree': sp_uniform(0.5, 0.5)\n",
    "}\n",
    "\n",
    "param_dist_rf = {\n",
    "    'n_estimators': sp_randint(50, 300),\n",
    "    'max_depth': sp_randint(3, 20),\n",
    "    'min_samples_split': sp_randint(2, 10),\n",
    "    'min_samples_leaf': sp_randint(1, 5)\n",
    "}\n",
    "\n",
    "param_dist_gb = {\n",
    "    'learning_rate': sp_uniform(0.01, 0.3),\n",
    "    'n_estimators': sp_randint(50, 300),\n",
    "    'subsample': sp_uniform(0.5, 0.5),\n",
    "    'max_depth': sp_randint(3, 10)\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled) / sum(y_train_resampled))\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "mlp_classifier = MLPClassifier(random_state=42, max_iter=200)  # Reduce the number of iterations\n",
    "svm_classifier = SVC(probability=True, random_state=42, class_weight='balanced', max_iter=200)  # Add max_iter\n",
    "\n",
    "# Initialize RandomizedSearchCV for each classifier\n",
    "random_search_xgb = RandomizedSearchCV(estimator=xgb_classifier, param_distributions=param_dist_xgb, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search_rf = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_dist_rf, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search_gb = RandomizedSearchCV(estimator=gb_classifier, param_distributions=param_dist_gb, n_iter=50, cv=3, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV to the data\n",
    "random_search_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "random_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "random_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimators\n",
    "best_xgb_classifier = random_search_xgb.best_estimator_\n",
    "best_rf_classifier = random_search_rf.best_estimator_\n",
    "best_gb_classifier = random_search_gb.best_estimator_\n",
    "\n",
    "# Define the base learners\n",
    "base_learners = [\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', best_rf_classifier),\n",
    "    ('gb', best_gb_classifier),\n",
    "    ('mlp', mlp_classifier),\n",
    "    ('svm', svm_classifier)\n",
    "]\n",
    "\n",
    "# Define the meta-learner\n",
    "meta_learner = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Create the stacking ensemble\n",
    "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_learner, cv=3)\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Stacking Model Accuracy:\", accuracy)\n",
    "print(\"Stacking Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = stacking_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "adjusted_classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Adjusted Threshold:\", threshold)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", adjusted_classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32dd8a-b9cf-4837-9d2c-1d0a9f11aaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef7205-071b-44a1-95ad-3bc5728d2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a52470-a0ce-4917-a58c-cfc5a79078f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve, roc_auc_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Encode categorical features\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop('readmitted', axis=1)\n",
    "y = df['readmitted']\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Balance the dataset using SMOTE-Tomek\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Cost-sensitive XGBoost\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled) / sum(y_train_resampled))\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid_xgb, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_xgb_classifier = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Initialize other classifiers with cost-sensitive learning\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "lgb_classifier = LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Create an ensemble of models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', rf_classifier),\n",
    "    ('gb', gb_classifier),\n",
    "    ('lr', lr_classifier),\n",
    "    ('lgb', lgb_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model using ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "\n",
    "# Function to find the best threshold\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    ix = np.argmax(fscore)\n",
    "    best_threshold = thresholds[ix]\n",
    "    return best_threshold, precision[ix], recall[ix], fscore[ix]\n",
    "\n",
    "# Find the best threshold\n",
    "best_threshold, best_precision, best_recall, best_fscore = find_best_threshold(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "adjusted_classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", adjusted_classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb4f03-6f7f-4959-91f7-ce9c47a342a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410b5bb-22b9-4657-97d6-1d176018bc08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204ad7e-18e2-49bd-9947-95924bee5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1f6227-7846-4dfa-a3b6-07a33e191135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   5.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   5.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   7.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   7.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   7.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   7.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   6.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   7.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   5.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   6.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   5.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   6.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   5.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   7.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   2.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   6.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   5.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   2.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   5.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   5.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   7.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   2.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   5.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   5.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   4.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   5.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   8.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   3.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   5.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   5.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   3.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   7.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   4.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   3.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   5.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   8.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   5.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   5.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   5.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   8.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   8.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   6.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   2.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   6.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   2.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   2.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   5.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   6.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.7s\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   3.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   2.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  14.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   5.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  14.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  28.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   4.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  14.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  14.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  14.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  14.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   3.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   3.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   4.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   6.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  28.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   3.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   4.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   4.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   5.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  42.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  28.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  27.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  41.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  13.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  41.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  27.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  27.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  26.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  38.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  16.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  16.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  27.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   8.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   9.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  17.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  17.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  25.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  12.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  37.6s\n",
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  40.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  39.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   8.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  17.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  18.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  17.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  25.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   8.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  25.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  17.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  17.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  38.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  25.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  24.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  36.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  11.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  35.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  23.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=  42.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time= 2.2min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.9min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.0min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 2.1min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  34.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  31.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.2min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time= 1.6min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time= 2.8min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 2.5min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=  35.0s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=  32.4s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=  40.8s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time= 2.2min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time= 2.2min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.2min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time= 2.3min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  29.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  13.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  13.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  41.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  14.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  14.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  27.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  39.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   8.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   8.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   8.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   8.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  25.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  17.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  16.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  25.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   8.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  25.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  25.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  24.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  36.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  12.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  36.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  23.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  36.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=  39.1s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time= 1.6min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  56.7s\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time= 3.2min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time= 3.3min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.4min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  50.7s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.7min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time= 3.2min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time= 3.5min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time= 1.8min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=  51.7s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time= 2.4min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.2min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time= 3.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  28.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  41.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  13.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  42.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  26.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  13.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  26.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  25.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   8.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  25.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   8.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  25.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  18.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  27.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  12.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  25.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  37.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  12.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  12.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  25.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  23.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  35.0s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time= 2.0min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time= 2.4min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.2min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time= 3.1min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.4min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  55.2s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.7min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time= 3.2min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time= 4.7min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=  51.8s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.9min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.2min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.4min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.8min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  24.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  36.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  12.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  12.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  24.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  23.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  30.3s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.2min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=  51.4s\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  56.9s\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time= 2.6min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 2.0min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time= 3.7min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  50.7s\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 2.3min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.1min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 2.3min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time= 4.4min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time= 2.2min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.6min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.8min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  28.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  14.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  27.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  28.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  41.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  26.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  26.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  25.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   8.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   9.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   8.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   9.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  16.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  17.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  25.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   8.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  25.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  25.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  36.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  25.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  36.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  11.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  34.7s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=  39.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time= 1.6min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time= 2.9min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time= 3.2min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time= 1.8min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time= 2.6min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.4min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time= 3.5min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.4min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time= 2.0min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time= 3.5min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time= 3.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  42.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  41.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  14.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  13.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  26.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  26.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  39.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   8.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  25.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  17.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  25.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   8.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   9.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  18.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  26.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  37.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  12.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  37.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  24.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  37.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  12.2s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  35.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time= 1.8min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time= 2.9min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time= 4.1min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time= 2.0min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time= 2.6min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 2.3min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time= 4.5min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time= 2.6min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.4min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time= 3.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  42.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  13.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  13.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  28.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  26.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  39.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  12.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  39.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   9.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=300; total time=  25.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  18.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  27.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   8.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   9.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  18.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  25.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  37.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  12.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=  12.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  24.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  24.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  35.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  11.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  29.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time= 2.0min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time= 2.4min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 2.1min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time= 3.8min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time= 1.2min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 2.1min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.5min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.9min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=  44.4s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time= 1.7min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.7min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time= 3.2min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time= 3.2min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  43.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  28.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  27.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  41.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  13.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  13.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=300; total time=  39.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  16.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  27.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   8.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  25.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  17.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  27.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   8.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  13.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  37.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  24.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  38.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  12.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  36.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  23.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  32.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  32.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.3min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=  49.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 2.1min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.0min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.5min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  32.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  39.9s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.5min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time= 2.0min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time= 3.5min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time= 3.8min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.4min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=  56.0s\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.7min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time= 3.3min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time= 4.1min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  43.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  27.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  27.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  41.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  13.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  38.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  26.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=  27.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  18.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  18.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  27.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   8.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=300; total time=  25.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  17.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=  25.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=  12.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  12.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  24.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=  24.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  37.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  11.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  12.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  22.5s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  32.6s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.3min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time= 1.0min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.9min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.2min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.3min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  43.1s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time= 1.6min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.9min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.2min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.4min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.9min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=  40.9s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.5min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time= 2.0min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time= 2.8min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time= 2.3min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time= 3.5min\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  43.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  41.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=  13.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=  40.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  26.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  40.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  17.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  17.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=300; total time=  27.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   8.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   9.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=  17.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  17.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=300; total time=  25.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  12.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=300; total time=  38.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  25.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=300; total time=  37.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  11.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=  12.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=  24.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=  22.4s\n",
      "[CV] END learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.5min\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=  49.0s\n",
      "[CV] END learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.7min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.1min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.2min\n",
      "[CV] END learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.3min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  39.7s\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n",
      "[CV] END learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time= 2.2min\n",
      "[CV] END learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 2.1min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.1min\n",
      "[CV] END learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 3.1min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=  32.7s\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.2min\n",
      "[CV] END learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time= 1.7min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time= 1.2min\n",
      "[CV] END learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time= 2.6min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time= 2.5min\n",
      "[CV] END learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time= 3.5min\n",
      "Ensemble Model Accuracy: 0.8769586954333456\n",
      "Ensemble Model Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93     26362\n",
      "           1       0.37      0.09      0.14      3441\n",
      "\n",
      "    accuracy                           0.88     29803\n",
      "   macro avg       0.63      0.53      0.54     29803\n",
      "weighted avg       0.83      0.88      0.84     29803\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Threshold: 0.3\n",
      "Adjusted Accuracy: 0.7386504714290507\n",
      "Adjusted Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.78      0.84     26362\n",
      "           1       0.21      0.44      0.28      3441\n",
      "\n",
      "    accuracy                           0.74     29803\n",
      "   macro avg       0.56      0.61      0.56     29803\n",
      "weighted avg       0.83      0.74      0.78     29803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_curve\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define categories for 'medical_specialty'\n",
    "high_frequency = {'InternalMedicine', 'Family/GeneralPractice'}\n",
    "low_frequency = {'Cardiology', 'Endocrinology'}\n",
    "pediatrics = {'Pediatrics'}\n",
    "psychic = {'Psychiatry', 'Psychology'}\n",
    "neurology = {'Neurology'}\n",
    "surgery = {'Surgery-General', 'Orthopedics'}\n",
    "ungrouped = {'Emergency/Trauma', 'ObstetricsandGynecology'}\n",
    "\n",
    "# Function to categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Function to map A1Cresult values\n",
    "def map_A1Cresult(value):\n",
    "    if value == '>8':\n",
    "        return 3\n",
    "    elif value == 'Norm':\n",
    "        return 0\n",
    "    elif value == '>7':\n",
    "        return 2\n",
    "    elif value == '>6':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'diabetic_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr', 'payer_code'])\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Convert age and weight ranges to average values\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "weight_mapping = {\n",
    "    '[0-25)': 12.5, '[25-50)': 37.5, '[50-75)': 62.5, '[75-100)': 87.5,\n",
    "    '[100-125)': 112.5, '[125-150)': 137.5, '[150-175)': 162.5,\n",
    "    '[175-200)': 187.5, '>200': 225\n",
    "}\n",
    "df['age'] = df['age'].map(age_mapping)\n",
    "df['weight'] = df['weight'].map(weight_mapping)\n",
    "\n",
    "# Fill NaN values in weight with the mean\n",
    "df['weight'] = df['weight'].fillna(df['weight'].mean())\n",
    "\n",
    "# Filter out specific discharge_disposition_id values\n",
    "excluded_dispositions = [11, 13, 14, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(excluded_dispositions)]\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "df['medical_specialty'] = df['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Handle missing values\n",
    "# Fill numerical columns with mean\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "\n",
    "# Fill specific medication columns with 0 if NaN and 1 if not\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', \n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "\n",
    "# Explicitly fill remaining NaNs in categorical columns with mode again\n",
    "df = df.apply(lambda x: x.fillna(x.mode()[0]), axis=0)\n",
    "\n",
    "# Map A1Cresult values to categories\n",
    "df['A1Cresult'] = df['A1Cresult'].apply(map_A1Cresult)\n",
    "\n",
    "# Encode the target variable\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Feature Engineering\n",
    "df['num_medications_age'] = df['num_medications'] * df['age']\n",
    "df['num_lab_procedures_num_medications'] = df['num_lab_procedures'] * df['num_medications']\n",
    "df['num_medications_time_in_hospital'] = df['num_medications'] * df['time_in_hospital']\n",
    "df['num_procedures_time_in_hospital'] = df['num_procedures'] * df['time_in_hospital']\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id', \n",
    "    'admission_source_id', 'max_glu_serum', 'change', 'diabetesMed', 'medical_specialty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all columns are numerical and fill any remaining NaNs\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['readmitted'])\n",
    "y = df['readmitted']\n",
    "\n",
    "# Encode remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE-Tomek to the training data\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier with adjusted scale_pos_weight\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42, scale_pos_weight=len(y_train_resampled)/sum(y_train_resampled) * 1.5)\n",
    "rf_classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Initialize GridSearchCV for each classifier\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid_xgb, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_rf = GridSearchCV(estimator=rf_classifier, param_grid=param_grid_rf, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_gb = GridSearchCV(estimator=gb_classifier, param_grid=param_grid_gb, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimators\n",
    "best_xgb_classifier = grid_search_xgb.best_estimator_\n",
    "best_rf_classifier = grid_search_rf.best_estimator_\n",
    "best_gb_classifier = grid_search_gb.best_estimator_\n",
    "\n",
    "# Create an ensemble of models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('xgb', best_xgb_classifier),\n",
    "    ('rf', best_rf_classifier),\n",
    "    ('gb', best_gb_classifier),\n",
    "    ('lr', lr_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the ensemble model\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Ensemble Model Accuracy:\", accuracy)\n",
    "print(\"Ensemble Model Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Make predictions on the test set probabilities\n",
    "y_pred_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall Scores as a function of the decision threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold that improves recall\n",
    "threshold = 0.3  # Adjust this value based on the plot\n",
    "\n",
    "# Make predictions based on the new threshold\n",
    "y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "adjusted_accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "adjusted_classification_rep = classification_report(y_test, y_pred_adjusted)\n",
    "\n",
    "print(\"Adjusted Threshold:\", threshold)\n",
    "print(\"Adjusted Accuracy:\", adjusted_accuracy)\n",
    "print(\"Adjusted Classification Report:\\n\", adjusted_classification_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f07146-8d37-4320-8f94-9749a7ad355c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
