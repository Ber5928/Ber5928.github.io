{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e233236a-8b8b-4cc9-a782-572f454b1ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 29, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 8, 'n_estimators': 70}\n",
      "Best ROC-AUC score for Random Forest: 0.9708095936187945\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8863761998090356\n",
      "ROC-AUC: 0.5113573492136174\n",
      "Precision: 0.4105960264900662\n",
      "Recall: 0.02775290957923008\n",
      "F1: 0.0519916142557652\n",
      "Balanced Accuracy: 0.5113573492136173\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score, confusion_matrix\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = Pipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid):\n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=20, cv=3, scoring='roc_auc', n_jobs=1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "accuracy, roc_auc, precision, recall, f1, balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used LassoCV for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using the SMOTETomek technique to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built a Random Forest classifier and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the model using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Model Performance: The Random Forest model with the best hyperparameters showed good performance on the validation set, with the following metrics:\n",
    "# - Accuracy: [specific value]\n",
    "# - ROC-AUC: [specific value]\n",
    "# - Precision: [specific value]\n",
    "# - Recall: [specific value]\n",
    "# - F1 Score: [specific value]\n",
    "# - Balanced Accuracy: [specific value]\n",
    "\n",
    "# Feature Importance: The feature selection using LassoCV highlighted key features that significantly impact the prediction of patient readmission. These included interactions between medications and age, as well as the number of lab procedures.\n",
    "\n",
    "# Handling Imbalance: The use of SMOTETomek effectively balanced the classes in the training data, improving the model's ability to generalize and perform well on the minority class.\n",
    "\n",
    "# Hyperparameter Tuning: The hyperparameter tuning process was crucial in optimizing the model, demonstrating that careful selection of model parameters can substantially enhance performance.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "# The model building and evaluation process revealed that a well-tuned Random Forest classifier, combined with effective feature selection and class balancing techniques, can provide valuable predictions for patient readmission within 30 days. These insights can aid healthcare providers in identifying high-risk patients and implementing early interventions to reduce readmission rates. Future work could involve further refining the model, exploring additional features, and validating the model on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05db2fa-3463-4cce-8778-3eed68bcb1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 29, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 8, 'n_estimators': 70}\n",
      "Best ROC-AUC score for Random Forest: 0.9708095936187945\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8863761998090356\n",
      "ROC-AUC: 0.5113573492136174\n",
      "Precision: 0.4105960264900662\n",
      "Recall: 0.02775290957923008\n",
      "F1: 0.0519916142557652\n",
      "Balanced Accuracy: 0.5113573492136173\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score, confusion_matrix\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = Pipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid):\n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=20, cv=3, scoring='roc_auc', n_jobs=1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "accuracy, roc_auc, precision, recall, f1, balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used LassoCV for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using the SMOTETomek technique to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built a Random Forest classifier and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the model using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Model Performance: The Random Forest model with the best hyperparameters showed good performance on the validation set, with the following metrics:\n",
    "# - Accuracy: 0.886\n",
    "# - ROC-AUC: 0.511\n",
    "# - Precision: 0.411\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75bdaf5a-b9fc-4c5b-85f6-6181fe9fd3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "Best ROC-AUC score for Random Forest: 0.98310224050398\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8863259460274385\n",
      "ROC-AUC: 0.5101559892772101\n",
      "Precision: 0.4\n",
      "Recall: 0.025067144136078783\n",
      "F1: 0.04717775905644482\n",
      "Balanced Accuracy: 0.5101559892772101\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m\n\u001b[1;32m    111\u001b[0m param_grid_gb \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m],\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m    117\u001b[0m }\n\u001b[1;32m    119\u001b[0m grid_search_gb \u001b[38;5;241m=\u001b[39m GridSearchCV(gb, param_grid\u001b[38;5;241m=\u001b[39mparam_grid_gb, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m grid_search_gb\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Best Gradient Boosting model\u001b[39;00m\n\u001b[1;32m    123\u001b[0m best_gb \u001b[38;5;241m=\u001b[39m grid_search_gb\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = Pipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid):\n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf, param_grid=param_grid_rf, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1, error_score='raise')\n",
    "grid_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", grid_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with GridSearchCV\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(gb, param_grid=param_grid_gb, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1, error_score='raise')\n",
    "grid_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = grid_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", grid_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", grid_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used LassoCV for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using the SMOTETomek technique to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using GridSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.886\n",
    "# - ROC-AUC: 0.511\n",
    "# - Precision: 0.411\n",
    "# - Recall: 0.028\n",
    "# - F1 Score: 0.052\n",
    "# - Balanced Accuracy: 0.511\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# The model building and evaluation process revealed that further improvements are needed, particularly in recall and F1 score, to create a reliable predictive tool for patient readmission within 30 days. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d3a185c-1cc4-4e24-b1c9-9f0a7dde436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 22, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
      "Best ROC-AUC score for Random Forest: 0.9749788638768021\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8854213779586914\n",
      "ROC-AUC: 0.5116015995495596\n",
      "Precision: 0.3707865168539326\n",
      "Recall: 0.02954341987466428\n",
      "F1: 0.054726368159203974\n",
      "Balanced Accuracy: 0.5116015995495597\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 121\u001b[0m\n\u001b[1;32m    112\u001b[0m param_dist_gb \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    113\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m150\u001b[39m),\n\u001b[1;32m    114\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: uniform(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    118\u001b[0m }\n\u001b[1;32m    120\u001b[0m random_search_gb \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(gb, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist_gb, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m random_search_gb\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Best Gradient Boosting model\u001b[39;00m\n\u001b[1;32m    124\u001b[0m best_gb \u001b[38;5;241m=\u001b[39m random_search_gb\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = Pipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid):\n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Ensemble Model with Voting Classifier\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('rf', best_rf),\n",
    "    ('gb', best_gb)\n",
    "], voting='soft', n_jobs=-1)\n",
    "\n",
    "ensemble_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Ensemble Model on the validation set\n",
    "ensemble_accuracy, ensemble_roc_auc, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_balanced_accuracy = evaluate_model(ensemble_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Ensemble Model with Voting Classifier:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy}\")\n",
    "print(f\"ROC-AUC: {ensemble_roc_auc}\")\n",
    "print(f\"Precision: {ensemble_precision}\")\n",
    "print(f\"Recall: {ensemble_recall}\")\n",
    "print(f\"F1: {ensemble_f1}\")\n",
    "print(f\"Balanced Accuracy: {ensemble_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Ensemble Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_prob = ensemble_model.predict_proba(X_valid_selected)[:, 1]\n",
    "    y_pred_adjusted = (y_pred_prob >= threshold).astype(int)\n",
    "    f1 = f1_score(y_valid, y_pred_adjusted)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used LassoCV for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using the SMOTETomek technique to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into an ensemble using Voting Classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.886\n",
    "# - ROC-AUC: 0.510\n",
    "# - Precision: 0.400\n",
    "# - Recall: 0.025\n",
    "# - F1 Score: 0.047\n",
    "# - Balanced Accuracy: 0.510\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# Ensemble Model Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into an ensemble and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d81176-bd89-4e14-a04c-8b022a7af8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 22, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
      "Best ROC-AUC score for Random Forest: 0.9742265993994615\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate_model() missing 1 required positional argument: 'y_valid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest ROC-AUC score for Random Forest:\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_search_rf\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Evaluate Random Forest on the validation set\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(best_rf, X_valid_selected)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest with Best Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrf_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_model() missing 1 required positional argument: 'y_valid'"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = Pipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Ensemble Model with Voting Classifier\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('rf', best_rf),\n",
    "    ('gb', best_gb)\n",
    "], voting='soft', n_jobs=-1)\n",
    "\n",
    "ensemble_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Ensemble Model on the validation set\n",
    "ensemble_accuracy, ensemble_roc_auc, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_balanced_accuracy = evaluate_model(ensemble_model, X_valid_selected)\n",
    "\n",
    "print(\"Ensemble Model with Voting Classifier:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy}\")\n",
    "print(f\"ROC-AUC: {ensemble_roc_auc}\")\n",
    "print(f\"Precision: {ensemble_precision}\")\n",
    "print(f\"Recall: {ensemble_recall}\")\n",
    "print(f\"F1: {ensemble_f1}\")\n",
    "print(f\"Balanced Accuracy: {ensemble_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Ensemble Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = ensemble_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(ensemble_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "ensemble_accuracy, ensemble_roc_auc, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_balanced_accuracy = evaluate_model(ensemble_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Ensemble Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy}\")\n",
    "print(f\"ROC-AUC: {ensemble_roc_auc}\")\n",
    "print(f\"Precision: {ensemble_precision}\")\n",
    "print(f\"Recall: {ensemble_recall}\")\n",
    "print(f\"F1: {ensemble_f1}\")\n",
    "print(f\"Balanced Accuracy: {ensemble_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used LassoCV for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using the SMOTETomek technique to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into an ensemble using Voting Classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.885\n",
    "# - ROC-AUC: 0.512\n",
    "# - Precision: 0.371\n",
    "# - Recall: 0.030\n",
    "# - F1 Score: 0.055\n",
    "# - Balanced Accuracy: 0.512\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# Ensemble Model Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into an ensemble and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54182d54-b044-4b71-897c-39d64d6f1849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 22, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
      "Best ROC-AUC score for Random Forest: 0.9742265993994615\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8847680787979295\n",
      "ROC-AUC: 0.6382037274339575\n",
      "Precision: 0.34554973821989526\n",
      "Recall: 0.02954341987466428\n",
      "F1: 0.05443298969072165\n",
      "Balanced Accuracy: 0.5112336403081218\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 123\u001b[0m\n\u001b[1;32m    114\u001b[0m param_dist_gb \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m150\u001b[39m),\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: uniform(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    120\u001b[0m }\n\u001b[1;32m    122\u001b[0m random_search_gb \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(gb, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist_gb, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m random_search_gb\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Best Gradient Boosting model\u001b[39;00m\n\u001b[1;32m    126\u001b[0m best_gb \u001b[38;5;241m=\u001b[39m random_search_gb\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = Pipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Ensemble Model with Voting Classifier\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('rf', best_rf),\n",
    "    ('gb', best_gb)\n",
    "], voting='soft', n_jobs=-1)\n",
    "\n",
    "ensemble_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Ensemble Model on the validation set\n",
    "ensemble_accuracy, ensemble_roc_auc, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_balanced_accuracy = evaluate_model(ensemble_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Ensemble Model with Voting Classifier:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy}\")\n",
    "print(f\"ROC-AUC: {ensemble_roc_auc}\")\n",
    "print(f\"Precision: {ensemble_precision}\")\n",
    "print(f\"Recall: {ensemble_recall}\")\n",
    "print(f\"F1: {ensemble_f1}\")\n",
    "print(f\"Balanced Accuracy: {ensemble_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Ensemble Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = ensemble_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(ensemble_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "ensemble_accuracy, ensemble_roc_auc, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_balanced_accuracy = evaluate_model(ensemble_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Ensemble Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy}\")\n",
    "print(f\"ROC-AUC: {ensemble_roc_auc}\")\n",
    "print(f\"Precision: {ensemble_precision}\")\n",
    "print(f\"Recall: {ensemble_recall}\")\n",
    "print(f\"F1: {ensemble_f1}\")\n",
    "print(f\"Balanced Accuracy: {ensemble_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used LassoCV for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using the SMOTETomek technique to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into an ensemble using Voting Classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.885\n",
    "# - ROC-AUC: 0.512\n",
    "# - Precision: 0.371\n",
    "# - Recall: 0.030\n",
    "# - F1 Score: 0.055\n",
    "# - Balanced Accuracy: 0.512\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# Ensemble Model Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into an ensemble and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc7ea5f-3e91-44d6-9bf4-4288d57a0ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 22, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
      "Best ROC-AUC score for Random Forest: 0.9742265993994615\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8847680787979295\n",
      "ROC-AUC: 0.6382037274339575\n",
      "Precision: 0.34554973821989526\n",
      "Recall: 0.02954341987466428\n",
      "F1: 0.05443298969072165\n",
      "Balanced Accuracy: 0.5112336403081218\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 123\u001b[0m\n\u001b[1;32m    114\u001b[0m param_dist_gb \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m150\u001b[39m),\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: uniform(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    120\u001b[0m }\n\u001b[1;32m    122\u001b[0m random_search_gb \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(gb, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist_gb, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m random_search_gb\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Best Gradient Boosting model\u001b[39;00m\n\u001b[1;32m    126\u001b[0m best_gb \u001b[38;5;241m=\u001b[39m random_search_gb\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = Pipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Ensemble Model with Voting Classifier\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('rf', best_rf),\n",
    "    ('gb', best_gb)\n",
    "], voting='soft', n_jobs=-1)\n",
    "\n",
    "ensemble_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Ensemble Model on the validation set\n",
    "ensemble_accuracy, ensemble_roc_auc, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_balanced_accuracy = evaluate_model(ensemble_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Ensemble Model with Voting Classifier:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy}\")\n",
    "print(f\"ROC-AUC: {ensemble_roc_auc}\")\n",
    "print(f\"Precision: {ensemble_precision}\")\n",
    "print(f\"Recall: {ensemble_recall}\")\n",
    "print(f\"F1: {ensemble_f1}\")\n",
    "print(f\"Balanced Accuracy: {ensemble_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Ensemble Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = ensemble_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(ensemble_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "ensemble_accuracy, ensemble_roc_auc, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_balanced_accuracy = evaluate_model(ensemble_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Ensemble Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy}\")\n",
    "print(f\"ROC-AUC: {ensemble_roc_auc}\")\n",
    "print(f\"Precision: {ensemble_precision}\")\n",
    "print(f\"Recall: {ensemble_recall}\")\n",
    "print(f\"F1: {ensemble_f1}\")\n",
    "print(f\"Balanced Accuracy: {ensemble_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used LassoCV for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using the SMOTETomek technique to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into an ensemble using Voting Classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.885\n",
    "# - ROC-AUC: 0.638\n",
    "# - Precision: 0.346\n",
    "# - Recall: 0.030\n",
    "# - F1 Score: 0.054\n",
    "# - Balanced Accuracy: 0.511\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# Ensemble Model Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into an ensemble and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115348e6-9735-4aab-8c65-60e136352063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 22, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 120}\n",
      "Best ROC-AUC score for Random Forest: 0.9750940242205476\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8814231284110802\n",
      "ROC-AUC: 0.6339813123924802\n",
      "Precision: 0.3211009174311927\n",
      "Recall: 0.03146067415730337\n",
      "F1: 0.05730659025787966\n",
      "Balanced Accuracy: 0.5114272609607242\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 127\u001b[0m\n\u001b[1;32m    118\u001b[0m param_dist_gb \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m150\u001b[39m),\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: uniform(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    124\u001b[0m }\n\u001b[1;32m    126\u001b[0m random_search_gb \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(gb, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist_gb, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m random_search_gb\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Best Gradient Boosting model\u001b[39;00m\n\u001b[1;32m    130\u001b[0m best_gb \u001b[38;5;241m=\u001b[39m random_search_gb\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = Pipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Ensemble Model with Voting Classifier\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('rf', best_rf),\n",
    "    ('gb', best_gb)\n",
    "], voting='soft', n_jobs=-1)\n",
    "\n",
    "ensemble_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Ensemble Model on the validation set\n",
    "ensemble_accuracy, ensemble_roc_auc, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_balanced_accuracy = evaluate_model(ensemble_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Ensemble Model with Voting Classifier:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy}\")\n",
    "print(f\"ROC-AUC: {ensemble_roc_auc}\")\n",
    "print(f\"Precision: {ensemble_precision}\")\n",
    "print(f\"Recall: {ensemble_recall}\")\n",
    "print(f\"F1: {ensemble_f1}\")\n",
    "print(f\"Balanced Accuracy: {ensemble_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Ensemble Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = ensemble_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(ensemble_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "ensemble_accuracy, ensemble_roc_auc, ensemble_precision, ensemble_recall, ensemble_f1, ensemble_balanced_accuracy = evaluate_model(ensemble_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Ensemble Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {ensemble_accuracy}\")\n",
    "print(f\"ROC-AUC: {ensemble_roc_auc}\")\n",
    "print(f\"Precision: {ensemble_precision}\")\n",
    "print(f\"Recall: {ensemble_recall}\")\n",
    "print(f\"F1: {ensemble_f1}\")\n",
    "print(f\"Balanced Accuracy: {ensemble_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used LassoCV for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using the SMOTETomek technique to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into an ensemble using Voting Classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.885\n",
    "# - ROC-AUC: 0.638\n",
    "# - Precision: 0.346\n",
    "# - Recall: 0.030\n",
    "# - F1 Score: 0.054\n",
    "# - Balanced Accuracy: 0.511\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# Ensemble Model Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into an ensemble and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920b82d2-17a8-47b1-967b-f4f552cdf5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9419636545000533\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8681907115642056\n",
      "ROC-AUC: 0.6238769239212745\n",
      "Precision: 0.28104575163398693\n",
      "Recall: 0.09662921348314607\n",
      "F1: 0.14381270903010032\n",
      "Balanced Accuracy: 0.532323445492518\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.19789978831283783, 'max_depth': 8, 'min_samples_leaf': 4, 'min_samples_split': 7, 'n_estimators': 144}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9503087227072061\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8773040881474616\n",
      "ROC-AUC: 0.6285959205061243\n",
      "Precision: 0.28296703296703296\n",
      "Recall: 0.04629213483146068\n",
      "F1: 0.07956740054074933\n",
      "Balanced Accuracy: 0.5155575345320879\n",
      "Stacking Model:\n",
      "Accuracy: 0.8693234476367007\n",
      "ROC-AUC: 0.632658008680636\n",
      "Precision: 0.27926657263751764\n",
      "Recall: 0.08898876404494382\n",
      "F1: 0.13496932515337423\n",
      "Balanced Accuracy: 0.529637139480168\n",
      "Best threshold: 0.1, Best F1 Score: 0.2488138166635035\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.5924209659149419\n",
      "ROC-AUC: 0.632658008680636\n",
      "Precision: 0.15770479971129556\n",
      "Recall: 0.5892134831460674\n",
      "F1: 0.2488138166635035\n",
      "Balanced Accuracy: 0.5910247214532454\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.881\n",
    "# - ROC-AUC: 0.634\n",
    "# - Precision: 0.321\n",
    "# - Recall: 0.031\n",
    "# - F1 Score: 0.057\n",
    "# - Balanced Accuracy: 0.511\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: [value]\n",
    "# - ROC-AUC: [value]\n",
    "# - Precision: [value]\n",
    "# - Recall: [value]\n",
    "# - F1 Score: [value]\n",
    "# - Balanced Accuracy: [value]\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa531707-bb9d-4d3c-a957-003fa7739595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9419636545000533\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8681907115642056\n",
      "ROC-AUC: 0.6238769239212745\n",
      "Precision: 0.28104575163398693\n",
      "Recall: 0.09662921348314607\n",
      "F1: 0.14381270903010032\n",
      "Balanced Accuracy: 0.532323445492518\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.19789978831283783, 'max_depth': 8, 'min_samples_leaf': 4, 'min_samples_split': 7, 'n_estimators': 144}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9503087227072061\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8773040881474616\n",
      "ROC-AUC: 0.6285959205061243\n",
      "Precision: 0.28296703296703296\n",
      "Recall: 0.04629213483146068\n",
      "F1: 0.07956740054074933\n",
      "Balanced Accuracy: 0.5155575345320879\n",
      "Stacking Model:\n",
      "Accuracy: 0.8693234476367007\n",
      "ROC-AUC: 0.632658008680636\n",
      "Precision: 0.27926657263751764\n",
      "Recall: 0.08898876404494382\n",
      "F1: 0.13496932515337423\n",
      "Balanced Accuracy: 0.529637139480168\n",
      "Best threshold: 0.1, Best F1 Score: 0.2488138166635035\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.5924209659149419\n",
      "ROC-AUC: 0.632658008680636\n",
      "Precision: 0.15770479971129556\n",
      "Recall: 0.5892134831460674\n",
      "F1: 0.2488138166635035\n",
      "Balanced Accuracy: 0.5910247214532454\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.868\n",
    "# - ROC-AUC: 0.624\n",
    "# - Precision: 0.281\n",
    "# - Recall: 0.097\n",
    "# - F1 Score: 0.144\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.877\n",
    "# - ROC-AUC: 0.629\n",
    "# - Precision: 0.283\n",
    "# - Recall: 0.046\n",
    "# - F1 Score: 0.080\n",
    "# - Balanced Accuracy: 0.516\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.869\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.279\n",
    "# - Recall: 0.089\n",
    "# - F1 Score: 0.135\n",
    "# - Balanced Accuracy: 0.530\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.592\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.158\n",
    "# - Recall: 0.589\n",
    "# - F1 Score: 0.249\n",
    "# - Balanced Accuracy: 0.591\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e2c6b0-493c-4d2c-91b6-b76a3fada08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9419636545000533\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8681907115642056\n",
      "ROC-AUC: 0.6238769239212745\n",
      "Precision: 0.28104575163398693\n",
      "Recall: 0.09662921348314607\n",
      "F1: 0.14381270903010032\n",
      "Balanced Accuracy: 0.532323445492518\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.19789978831283783, 'max_depth': 8, 'min_samples_leaf': 4, 'min_samples_split': 7, 'n_estimators': 144}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9503087227072061\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8773040881474616\n",
      "ROC-AUC: 0.6285959205061243\n",
      "Precision: 0.28296703296703296\n",
      "Recall: 0.04629213483146068\n",
      "F1: 0.07956740054074933\n",
      "Balanced Accuracy: 0.5155575345320879\n",
      "Stacking Model:\n",
      "Accuracy: 0.8693234476367007\n",
      "ROC-AUC: 0.632658008680636\n",
      "Precision: 0.27926657263751764\n",
      "Recall: 0.08898876404494382\n",
      "F1: 0.13496932515337423\n",
      "Balanced Accuracy: 0.529637139480168\n",
      "Best threshold: 0.1, Best F1 Score: 0.2488138166635035\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.5924209659149419\n",
      "ROC-AUC: 0.632658008680636\n",
      "Precision: 0.15770479971129556\n",
      "Recall: 0.5892134831460674\n",
      "F1: 0.2488138166635035\n",
      "Balanced Accuracy: 0.5910247214532454\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.868\n",
    "# - ROC-AUC: 0.624\n",
    "# - Precision: 0.281\n",
    "# - Recall: 0.097\n",
    "# - F1 Score: 0.144\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.877\n",
    "# - ROC-AUC: 0.629\n",
    "# - Precision: 0.283\n",
    "# - Recall: 0.046\n",
    "# - F1 Score: 0.080\n",
    "# - Balanced Accuracy: 0.516\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.869\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.279\n",
    "# - Recall: 0.089\n",
    "# - F1 Score: 0.135\n",
    "# - Balanced Accuracy: 0.530\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.592\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.158\n",
    "# - Recall: 0.589\n",
    "# - F1 Score: 0.249\n",
    "# - Balanced Accuracy: 0.591\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c66873-5aaa-4c5b-8a8a-36d6fc15736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9419636545000533\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8681907115642056\n",
      "ROC-AUC: 0.6238769239212745\n",
      "Precision: 0.28104575163398693\n",
      "Recall: 0.09662921348314607\n",
      "F1: 0.14381270903010032\n",
      "Balanced Accuracy: 0.532323445492518\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.19789978831283783, 'max_depth': 8, 'min_samples_leaf': 4, 'min_samples_split': 7, 'n_estimators': 144}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9503087227072061\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8773040881474616\n",
      "ROC-AUC: 0.6285959205061243\n",
      "Precision: 0.28296703296703296\n",
      "Recall: 0.04629213483146068\n",
      "F1: 0.07956740054074933\n",
      "Balanced Accuracy: 0.5155575345320879\n",
      "Stacking Model:\n",
      "Accuracy: 0.8693234476367007\n",
      "ROC-AUC: 0.632658008680636\n",
      "Precision: 0.27926657263751764\n",
      "Recall: 0.08898876404494382\n",
      "F1: 0.13496932515337423\n",
      "Balanced Accuracy: 0.529637139480168\n",
      "Best threshold: 0.1, Best F1 Score: 0.2488138166635035\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.5924209659149419\n",
      "ROC-AUC: 0.632658008680636\n",
      "Precision: 0.15770479971129556\n",
      "Recall: 0.5892134831460674\n",
      "F1: 0.2488138166635035\n",
      "Balanced Accuracy: 0.5910247214532454\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.868\n",
    "# - ROC-AUC: 0.624\n",
    "# - Precision: 0.281\n",
    "# - Recall: 0.097\n",
    "# - F1 Score: 0.144\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.877\n",
    "# - ROC-AUC: 0.629\n",
    "# - Precision: 0.283\n",
    "# - Recall: 0.046\n",
    "# - F1 Score: 0.080\n",
    "# - Balanced Accuracy: 0.516\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.869\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.279\n",
    "# - Recall: 0.089\n",
    "# - F1 Score: 0.135\n",
    "# - Balanced Accuracy: 0.530\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.592\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.158\n",
    "# - Recall: 0.589\n",
    "# - F1 Score: 0.249\n",
    "# - Balanced Accuracy: 0.591\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc5532b-8a2a-45b3-b8cc-ac7f37cc00ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9602446784277509\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8815261044176707\n",
      "ROC-AUC: 0.6334922278709443\n",
      "Precision: 0.3592592592592593\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07775551102204409\n",
      "Balanced Accuracy: 0.5167678057251951\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.14318447132349935, 'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 7, 'n_estimators': 140}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9549438417961301\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8842034805890228\n",
      "ROC-AUC: 0.6544443275643191\n",
      "Precision: 0.4189189189189189\n",
      "Recall: 0.027865168539325844\n",
      "F1: 0.05225453013063633\n",
      "Balanced Accuracy: 0.5114321481470834\n",
      "Stacking Model:\n",
      "Accuracy: 0.8785398002265472\n",
      "ROC-AUC: 0.6490656914944011\n",
      "Precision: 0.3649193548387097\n",
      "Recall: 0.08134831460674158\n",
      "F1: 0.1330393237780228\n",
      "Balanced Accuracy: 0.5315155831334575\n",
      "Best threshold: 0.1, Best F1 Score: 0.2683118339876613\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.6641437545052002\n",
      "ROC-AUC: 0.6490656914944011\n",
      "Precision: 0.17877428998505232\n",
      "Recall: 0.5375280898876404\n",
      "F1: 0.2683118339876613\n",
      "Balanced Accuracy: 0.6090268814850774\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.868\n",
    "# - ROC-AUC: 0.624\n",
    "# - Precision: 0.281\n",
    "# - Recall: 0.097\n",
    "# - F1 Score: 0.144\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.877\n",
    "# - ROC-AUC: 0.629\n",
    "# - Precision: 0.283\n",
    "# - Recall: 0.046\n",
    "# - F1 Score: 0.080\n",
    "# - Balanced Accuracy: 0.516\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.869\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.279\n",
    "# - Recall: 0.089\n",
    "# - F1 Score: 0.135\n",
    "# - Balanced Accuracy: 0.530\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.592\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.158\n",
    "# - Recall: 0.589\n",
    "# - F1 Score: 0.249\n",
    "# - Balanced Accuracy: 0.591\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f71ba8c5-d0d8-4fed-a3b5-eabbdf1bd69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9602446784277509\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8815261044176707\n",
      "ROC-AUC: 0.6334922278709443\n",
      "Precision: 0.3592592592592593\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07775551102204409\n",
      "Balanced Accuracy: 0.5167678057251951\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.14318447132349935, 'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 7, 'n_estimators': 140}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9549438417961301\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8842034805890228\n",
      "ROC-AUC: 0.6544443275643191\n",
      "Precision: 0.4189189189189189\n",
      "Recall: 0.027865168539325844\n",
      "F1: 0.05225453013063633\n",
      "Balanced Accuracy: 0.5114321481470834\n",
      "Stacking Model:\n",
      "Accuracy: 0.8785398002265472\n",
      "ROC-AUC: 0.6490656914944011\n",
      "Precision: 0.3649193548387097\n",
      "Recall: 0.08134831460674158\n",
      "F1: 0.1330393237780228\n",
      "Balanced Accuracy: 0.5315155831334575\n",
      "Best threshold: 0.1, Best F1 Score: 0.2683118339876613\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.6641437545052002\n",
      "ROC-AUC: 0.6490656914944011\n",
      "Precision: 0.17877428998505232\n",
      "Recall: 0.5375280898876404\n",
      "F1: 0.2683118339876613\n",
      "Balanced Accuracy: 0.6090268814850774\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.359\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.654\n",
    "# - Precision: 0.419\n",
    "# - Recall: 0.028\n",
    "# - F1 Score: 0.052\n",
    "# - Balanced Accuracy: 0.511\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.879\n",
    "# - ROC-AUC: 0.649\n",
    "# - Precision: 0.365\n",
    "# - Recall: 0.081\n",
    "# - F1 Score: 0.133\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.664\n",
    "# - ROC-AUC: 0.649\n",
    "# - Precision: 0.179\n",
    "# - Recall: 0.538\n",
    "# - F1 Score: 0.268\n",
    "# - Balanced Accuracy: 0.609\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849a7272-3d69-4446-9951-399a0ca840e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9602446784277509\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8815261044176707\n",
      "ROC-AUC: 0.6334922278709443\n",
      "Precision: 0.3592592592592593\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07775551102204409\n",
      "Balanced Accuracy: 0.5167678057251951\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.14318447132349935, 'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 7, 'n_estimators': 140}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9549438417961301\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8842034805890228\n",
      "ROC-AUC: 0.6544443275643191\n",
      "Precision: 0.4189189189189189\n",
      "Recall: 0.027865168539325844\n",
      "F1: 0.05225453013063633\n",
      "Balanced Accuracy: 0.5114321481470834\n",
      "Stacking Model:\n",
      "Accuracy: 0.8785398002265472\n",
      "ROC-AUC: 0.6490656914944011\n",
      "Precision: 0.3649193548387097\n",
      "Recall: 0.08134831460674158\n",
      "F1: 0.1330393237780228\n",
      "Balanced Accuracy: 0.5315155831334575\n",
      "Best threshold: 0.1, Best F1 Score: 0.2683118339876613\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.6641437545052002\n",
      "ROC-AUC: 0.6490656914944011\n",
      "Precision: 0.17877428998505232\n",
      "Recall: 0.5375280898876404\n",
      "F1: 0.2683118339876613\n",
      "Balanced Accuracy: 0.6090268814850774\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.359\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.654\n",
    "# - Precision: 0.419\n",
    "# - Recall: 0.028\n",
    "# - F1 Score: 0.052\n",
    "# - Balanced Accuracy: 0.511\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.879\n",
    "# - ROC-AUC: 0.649\n",
    "# - Precision: 0.365\n",
    "# - Recall: 0.081\n",
    "# - F1 Score: 0.133\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.664\n",
    "# - ROC-AUC: 0.649\n",
    "# - Precision: 0.179\n",
    "# - Recall: 0.538\n",
    "# - F1 Score: 0.268\n",
    "# - Balanced Accuracy: 0.609\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd74c21-d5e5-4f43-bc0c-3b4fa3aadd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Stacking Model:\n",
      "Accuracy: 0.8781793842034806\n",
      "ROC-AUC: 0.6481899442873823\n",
      "Precision: 0.36310679611650487\n",
      "Recall: 0.08404494382022472\n",
      "F1: 0.13649635036496352\n",
      "Balanced Accuracy: 0.5324859248379487\n",
      "Best threshold: 0.1, Best F1 Score: 0.26422139248846915\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.6550303779219442\n",
      "ROC-AUC: 0.6481899442873823\n",
      "Precision: 0.1748292399360558\n",
      "Recall: 0.5406741573033708\n",
      "F1: 0.26422139248846915\n",
      "Balanced Accuracy: 0.6052501448841678\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Categorized 'medical_specialty' into meaningful groups.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.633\n",
    "# - Precision: 0.359\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.654\n",
    "# - Precision: 0.419\n",
    "# - Recall: 0.028\n",
    "# - F1 Score: 0.052\n",
    "# - Balanced Accuracy: 0.511\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.879\n",
    "# - ROC-AUC: 0.649\n",
    "# - Precision: 0.365\n",
    "# - Recall: 0.081\n",
    "# - F1 Score: 0.133\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.664\n",
    "# - ROC-AUC: 0.649\n",
    "# - Precision: 0.179\n",
    "# - Recall: 0.538\n",
    "# - F1 Score: 0.268\n",
    "# - Balanced Accuracy: 0.609\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67bad6-d535-403e-abf3-13df0bb81b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1b2092-912f-4676-b5f1-62eb52a1549b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Stacking Model:\n",
      "Accuracy: 0.8781793842034806\n",
      "ROC-AUC: 0.6481899442873823\n",
      "Precision: 0.36310679611650487\n",
      "Recall: 0.08404494382022472\n",
      "F1: 0.13649635036496352\n",
      "Balanced Accuracy: 0.5324859248379487\n",
      "Best threshold: 0.1, Best F1 Score: 0.26422139248846915\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.6550303779219442\n",
      "ROC-AUC: 0.6481899442873823\n",
      "Precision: 0.1748292399360558\n",
      "Recall: 0.5406741573033708\n",
      "F1: 0.26422139248846915\n",
      "Balanced Accuracy: 0.6052501448841678\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Categorized 'medical_specialty' into meaningful groups.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.635\n",
    "# - Precision: 0.383\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.653\n",
    "# - Precision: 0.435\n",
    "# - Recall: 0.033\n",
    "# - F1 Score: 0.062\n",
    "# - Balanced Accuracy: 0.514\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.878\n",
    "# - ROC-AUC: 0.648\n",
    "# - Precision: 0.363\n",
    "# - Recall: 0.084\n",
    "# - F1 Score: 0.136\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.655\n",
    "# - ROC-AUC: 0.648\n",
    "# - Precision: 0.175\n",
    "# - Recall: 0.541\n",
    "# - F1 Score: 0.264\n",
    "# - Balanced Accuracy: 0.605\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed19e1a5-a41d-48e4-b17d-8fbdd3158e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Stacking Model:\n",
      "Accuracy: 0.8781793842034806\n",
      "ROC-AUC: 0.6481899442873823\n",
      "Precision: 0.36310679611650487\n",
      "Recall: 0.08404494382022472\n",
      "F1: 0.13649635036496352\n",
      "Balanced Accuracy: 0.5324859248379487\n",
      "Best threshold: 0.1, Best F1 Score: 0.26422139248846915\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.6550303779219442\n",
      "ROC-AUC: 0.6481899442873823\n",
      "Precision: 0.1748292399360558\n",
      "Recall: 0.5406741573033708\n",
      "F1: 0.26422139248846915\n",
      "Balanced Accuracy: 0.6052501448841678\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Categorized 'medical_specialty' into meaningful groups.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.635\n",
    "# - Precision: 0.383\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.653\n",
    "# - Precision: 0.435\n",
    "# - Recall: 0.033\n",
    "# - F1 Score: 0.062\n",
    "# - Balanced Accuracy: 0.514\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.878\n",
    "# - ROC-AUC: 0.648\n",
    "# - Precision: 0.363\n",
    "# - Recall: 0.084\n",
    "# - F1 Score: 0.136\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.655\n",
    "# - ROC-AUC: 0.648\n",
    "# - Precision: 0.175\n",
    "# - Recall: 0.541\n",
    "# - F1 Score: 0.264\n",
    "# - Balanced Accuracy: 0.605\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559cfb2c-fa96-4068-997c-d21b0a49783a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Stacking Model:\n",
      "Accuracy: 0.8781793842034806\n",
      "ROC-AUC: 0.6481899442873823\n",
      "Precision: 0.36310679611650487\n",
      "Recall: 0.08404494382022472\n",
      "F1: 0.13649635036496352\n",
      "Balanced Accuracy: 0.5324859248379487\n",
      "Best threshold: 0.1, Best F1 Score: 0.26422139248846915\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.6550303779219442\n",
      "ROC-AUC: 0.6481899442873823\n",
      "Precision: 0.1748292399360558\n",
      "Recall: 0.5406741573033708\n",
      "F1: 0.26422139248846915\n",
      "Balanced Accuracy: 0.6052501448841678\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Categorized 'medical_specialty' into meaningful groups.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.635\n",
    "# - Precision: 0.383\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.653\n",
    "# - Precision: 0.435\n",
    "# - Recall: 0.033\n",
    "# - F1 Score: 0.062\n",
    "# - Balanced Accuracy: 0.514\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.878\n",
    "# - ROC-AUC: 0.648\n",
    "# - Precision: 0.363\n",
    "# - Recall: 0.084\n",
    "# - F1 Score: 0.136\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.655\n",
    "# - ROC-AUC: 0.648\n",
    "# - Precision: 0.175\n",
    "# - Recall: 0.541\n",
    "# - F1 Score: 0.264\n",
    "# - Balanced Accuracy: 0.605\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bfe2b-4d55-4be6-af7c-0393e3918ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91afaca-4fb9-479a-801b-c55e4589b2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f603c347-6bc7-41fc-bd93-afe448fa9c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Stacking Model:\n",
      "Accuracy: 0.8781793842034806\n",
      "ROC-AUC: 0.6481899442873823\n",
      "Precision: 0.36310679611650487\n",
      "Recall: 0.08404494382022472\n",
      "F1: 0.13649635036496352\n",
      "Balanced Accuracy: 0.5324859248379487\n",
      "Best threshold: 0.1, Best F1 Score: 0.26422139248846915\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.6550303779219442\n",
      "ROC-AUC: 0.6481899442873823\n",
      "Precision: 0.1748292399360558\n",
      "Recall: 0.5406741573033708\n",
      "F1: 0.26422139248846915\n",
      "Balanced Accuracy: 0.6052501448841678\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Categorized 'medical_specialty' into meaningful groups.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest and Gradient Boosting classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.635\n",
    "# - Precision: 0.383\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.653\n",
    "# - Precision: 0.435\n",
    "# - Recall: 0.033\n",
    "# - F1 Score: 0.062\n",
    "# - Balanced Accuracy: 0.514\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.878\n",
    "# - ROC-AUC: 0.648\n",
    "# - Precision: 0.363\n",
    "# - Recall: 0.084\n",
    "# - F1 Score: 0.136\n",
    "# - Balanced Accuracy: 0.532\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.655\n",
    "# - ROC-AUC: 0.648\n",
    "# - Precision: 0.175\n",
    "# - Recall: 0.541\n",
    "# - F1 Score: 0.264\n",
    "# - Balanced Accuracy: 0.605\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1371b594-c25f-48c9-bed4-d9f8599cc8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'bagging_temperature': 0.7853406511139436, 'border_count': 135, 'depth': 6, 'iterations': 148, 'l2_leaf_reg': 4.722827665617431, 'learning_rate': 0.1980266884915557}\n",
      "Best ROC-AUC score for CatBoost: 0.9551220734583445\n",
      "CatBoost with Best Parameters:\n",
      "Accuracy: 0.8852332406549274\n",
      "ROC-AUC: 0.661459295552595\n",
      "Precision: 0.48484848484848486\n",
      "Recall: 0.028764044943820226\n",
      "F1: 0.054306321595248196\n",
      "Balanced Accuracy: 0.5124049334447542\n",
      "Stacking Model:\n",
      "Accuracy: 0.879621048295747\n",
      "ROC-AUC: 0.6546187896634701\n",
      "Precision: 0.37901498929336186\n",
      "Recall: 0.07955056179775281\n",
      "F1: 0.13150074294205052\n",
      "Balanced Accuracy: 0.5313435776948292\n",
      "Best threshold: 0.1, Best F1 Score: 0.26934540236229165\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.659200906188858\n",
      "ROC-AUC: 0.6546187896634701\n",
      "Precision: 0.17851916886157448\n",
      "Recall: 0.5483146067415731\n",
      "F1: 0.26934540236229165\n",
      "Balanced Accuracy: 0.6109311592758863\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define specialty categories\n",
    "# (Assuming the definition of high_frequency, low_frequency, pediatrics, psychic, neurology, surgery, ungrouped, missing remains the same)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model\n",
    "cat = CatBoostClassifier(random_state=42, silent=True)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost with RandomizedSearchCV\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'depth': randint(3, 10),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 255),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb439580-2b57-4cbb-89d9-ad5781f4e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'bagging_temperature': 0.7853406511139436, 'border_count': 135, 'depth': 6, 'iterations': 148, 'l2_leaf_reg': 4.722827665617431, 'learning_rate': 0.1980266884915557}\n",
      "Best ROC-AUC score for CatBoost: 0.9551220734583445\n",
      "CatBoost with Best Parameters:\n",
      "Accuracy: 0.8852332406549274\n",
      "ROC-AUC: 0.661459295552595\n",
      "Precision: 0.48484848484848486\n",
      "Recall: 0.028764044943820226\n",
      "F1: 0.054306321595248196\n",
      "Balanced Accuracy: 0.5124049334447542\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempt to pop from an empty stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 986, in cross_val_predict\n    predictions = parallel(\n                  ^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1098, in __call__\n    self.retrieve()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 975, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 774, in get\n    raise self._value\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1068, in _fit_and_predict\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 5201, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2374, in _fit\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 241\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Stacking Classifier with meta-classifier\u001b[39;00m\n\u001b[1;32m    231\u001b[0m stacking_model \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[1;32m    232\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    233\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, best_rf),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    239\u001b[0m )\n\u001b[0;32m--> 241\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Evaluate Stacking Model on the validation set\u001b[39;00m\n\u001b[1;32m    244\u001b[0m stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m    249\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 252\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    254\u001b[0m             clone(est),\n\u001b[1;32m    255\u001b[0m             X,\n\u001b[1;32m    256\u001b[0m             y,\n\u001b[1;32m    257\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[1;32m    258\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[1;32m    259\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    260\u001b[0m             fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    261\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     meth\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempt to pop from an empty stack"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model\n",
    "cat = CatBoostClassifier(random_state=42, silent=True)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost with RandomizedSearchCV\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'depth': randint(3, 10),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 255),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Categorized 'medical_specialty' into meaningful groups.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest, Gradient Boosting, and CatBoost classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.635\n",
    "# - Precision: 0.383\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.653\n",
    "# - Precision: 0.435\n",
    "# - Recall: 0.033\n",
    "# - F1 Score: 0.062\n",
    "# - Balanced Accuracy: 0.514\n",
    "\n",
    "# CatBoost Performance:\n",
    "# - Accuracy: 0.885\n",
    "# - ROC-AUC: 0.661\n",
    "# - Precision: 0.485\n",
    "# - Recall: 0.029\n",
    "# - F1 Score: 0.054\n",
    "# - Balanced Accuracy: 0.512\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.880\n",
    "# - ROC-AUC: 0.655\n",
    "# - Precision: 0.379\n",
    "# - Recall: 0.080\n",
    "# - F1 Score: 0.132\n",
    "# - Balanced Accuracy: 0.531\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.659\n",
    "# - ROC-AUC: 0.655\n",
    "# - Precision: 0.179\n",
    "# - Recall: 0.548\n",
    "# - F1 Score: 0.269\n",
    "# - Balanced Accuracy: 0.611\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6759016-4b28-4c2f-8ea1-b7bf7c170980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a21b8a-45e4-4267-8b59-9757ce49bc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for CatBoost: {'bagging_temperature': 0.7853406511139436, 'border_count': 135, 'depth': 6, 'iterations': 148, 'l2_leaf_reg': 4.722827665617431, 'learning_rate': 0.1980266884915557}\n",
      "Best ROC-AUC score for CatBoost: 0.9551220734583445\n",
      "CatBoost with Best Parameters:\n",
      "Accuracy: 0.8852332406549274\n",
      "ROC-AUC: 0.661459295552595\n",
      "Precision: 0.48484848484848486\n",
      "Recall: 0.028764044943820226\n",
      "F1: 0.054306321595248196\n",
      "Balanced Accuracy: 0.5124049334447542\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flush'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 244\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 244\u001b[0m     stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Evaluate Stacking Model on the validation set\u001b[39;00m\n\u001b[1;32m    247\u001b[0m stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m    249\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 252\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    254\u001b[0m             clone(est),\n\u001b[1;32m    255\u001b[0m             X,\n\u001b[1;32m    256\u001b[0m             y,\n\u001b[1;32m    257\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[1;32m    258\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[1;32m    259\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    260\u001b[0m             fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    261\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     meth\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:556\u001b[0m, in \u001b[0;36mLokyBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    555\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39msubmit(SafeFunction(func))\n\u001b[1;32m    557\u001b[0m     future\u001b[38;5;241m.\u001b[39mget \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_future_result, future)\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/reusable_executor.py:176\u001b[0m, in \u001b[0;36m_ReusablePoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubmit\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_resize_lock:\n\u001b[0;32m--> 176\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msubmit(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:1149\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# Wake up queue management thread\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor_manager_thread_wakeup\u001b[38;5;241m.\u001b[39mwakeup()\n\u001b[0;32m-> 1149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_executor_running()\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:1123\u001b[0m, in \u001b[0;36mProcessPoolExecutor._ensure_executor_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processes_management_lock:\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_workers:\n\u001b[0;32m-> 1123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adjust_process_count()\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_executor_manager_thread()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:1111\u001b[0m, in \u001b[0;36mProcessPoolExecutor._adjust_process_count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mProcess(target\u001b[38;5;241m=\u001b[39m_process_worker, args\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m   1110\u001b[0m     p\u001b[38;5;241m.\u001b[39m_worker_exit_lock \u001b[38;5;241m=\u001b[39m worker_exit_lock\n\u001b[0;32m-> 1111\u001b[0m     p\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processes[p\u001b[38;5;241m.\u001b[39mpid] \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m   1113\u001b[0m mp\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjusted process count to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_workers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[(p\u001b[38;5;241m.\u001b[39mname,\u001b[38;5;250m \u001b[39mpid)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mpid,\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_processes\u001b[38;5;241m.\u001b[39mitems()]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1116\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/backend/process.py:32\u001b[0m, in \u001b[0;36mLokyProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_loky_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/backend/popen_loky_posix.py:42\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[0;32m---> 42\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flush'"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model\n",
    "cat = CatBoostClassifier(random_state=42, silent=True)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost with RandomizedSearchCV\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'depth': randint(3, 10),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 255),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Silence CatBoost logging for stacking\n",
    "import contextlib\n",
    "with contextlib.redirect_stdout(None):\n",
    "    stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Categorized 'medical_specialty' into meaningful groups.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest, Gradient Boosting, and CatBoost classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.635\n",
    "# - Precision: 0.383\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.653\n",
    "# - Precision: 0.435\n",
    "# - Recall: 0.033\n",
    "# - F1 Score: 0.062\n",
    "# - Balanced Accuracy: 0.514\n",
    "\n",
    "# CatBoost Performance:\n",
    "# - Accuracy: 0.885\n",
    "# - ROC-AUC: 0.661\n",
    "# - Precision: 0.485\n",
    "# - Recall: 0.029\n",
    "# - F1 Score: 0.054\n",
    "# - Balanced Accuracy: 0.512\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.880\n",
    "# - ROC-AUC: 0.655\n",
    "# - Precision: 0.379\n",
    "# - Recall: 0.080\n",
    "# - F1 Score: 0.132\n",
    "# - Balanced Accuracy: 0.531\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.659\n",
    "# - ROC-AUC: 0.655\n",
    "# - Precision: 0.179\n",
    "# - Recall: 0.548\n",
    "# - F1 Score: 0.269\n",
    "# - Balanced Accuracy: 0.611\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8aa7078-ff81-4dc7-871f-c6aefe69a38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'bagging_temperature': 0.7853406511139436, 'border_count': 135, 'depth': 6, 'iterations': 148, 'l2_leaf_reg': 4.722827665617431, 'learning_rate': 0.1980266884915557}\n",
      "Best ROC-AUC score for CatBoost: 0.9551220734583445\n",
      "CatBoost with Best Parameters:\n",
      "Accuracy: 0.8852332406549274\n",
      "ROC-AUC: 0.661459295552595\n",
      "Precision: 0.48484848484848486\n",
      "Recall: 0.028764044943820226\n",
      "F1: 0.054306321595248196\n",
      "Balanced Accuracy: 0.5124049334447542\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempt to pop from an empty stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 986, in cross_val_predict\n    predictions = parallel(\n                  ^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1098, in __call__\n    self.retrieve()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 975, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 774, in get\n    raise self._value\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1068, in _fit_and_predict\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 5201, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2374, in _fit\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 242\u001b[0m\n\u001b[1;32m    231\u001b[0m stacking_model \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[1;32m    232\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    233\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, best_rf),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    239\u001b[0m )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Fit the stacking model\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Evaluate Stacking Model on the validation set\u001b[39;00m\n\u001b[1;32m    245\u001b[0m stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m    249\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 252\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    254\u001b[0m             clone(est),\n\u001b[1;32m    255\u001b[0m             X,\n\u001b[1;32m    256\u001b[0m             y,\n\u001b[1;32m    257\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[1;32m    258\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[1;32m    259\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    260\u001b[0m             fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    261\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     meth\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempt to pop from an empty stack"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model\n",
    "cat = CatBoostClassifier(random_state=42, silent=True)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost with RandomizedSearchCV\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'depth': randint(3, 10),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 255),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Categorized 'medical_specialty' into meaningful groups.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest, Gradient Boosting, and CatBoost classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.635\n",
    "# - Precision: 0.383\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.653\n",
    "# - Precision: 0.435\n",
    "# - Recall: 0.033\n",
    "# - F1 Score: 0.062\n",
    "# - Balanced Accuracy: 0.514\n",
    "\n",
    "# CatBoost Performance:\n",
    "# - Accuracy: 0.885\n",
    "# - ROC-AUC: 0.661\n",
    "# - Precision: 0.485\n",
    "# - Recall: 0.029\n",
    "# - F1 Score: 0.054\n",
    "# - Balanced Accuracy: 0.512\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.880\n",
    "# - ROC-AUC: 0.655\n",
    "# - Precision: 0.379\n",
    "# - Recall: 0.080\n",
    "# - F1 Score: 0.132\n",
    "# - Balanced Accuracy: 0.531\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.659\n",
    "# - ROC-AUC: 0.655\n",
    "# - Precision: 0.179\n",
    "# - Recall: 0.548\n",
    "# - F1 Score: 0.269\n",
    "# - Balanced Accuracy: 0.611\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efbad33d-10f1-4dc1-9048-a110ef9a70d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for CatBoost: {'bagging_temperature': 0.7853406511139436, 'border_count': 135, 'depth': 6, 'iterations': 148, 'l2_leaf_reg': 4.722827665617431, 'learning_rate': 0.1980266884915557}\n",
      "Best ROC-AUC score for CatBoost: 0.9551220734583445\n",
      "CatBoost with Best Parameters:\n",
      "Accuracy: 0.8852332406549274\n",
      "ROC-AUC: 0.661459295552595\n",
      "Precision: 0.48484848484848486\n",
      "Recall: 0.028764044943820226\n",
      "F1: 0.054306321595248196\n",
      "Balanced Accuracy: 0.5124049334447542\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempt to pop from an empty stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 184, in log_fixup\n    yield\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2433, in _fit\n    self.get_feature_importance(type=EFstrType.PredictionValuesChange)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 3179, in get_feature_importance\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 986, in cross_val_predict\n    predictions = parallel(\n                  ^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1098, in __call__\n    self.retrieve()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 975, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 774, in get\n    raise self._value\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1068, in _fit_and_predict\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/var/folders/yd/8fns7t4j1n9gv77y0dvs5zdc0000gn/T/ipykernel_70231/1336755681.py\", line 205, in fit\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 5201, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2374, in _fit\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 250\u001b[0m\n\u001b[1;32m    239\u001b[0m stacking_model \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[1;32m    240\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    241\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, best_rf),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Fit the stacking model\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Evaluate Stacking Model on the validation set\u001b[39;00m\n\u001b[1;32m    253\u001b[0m stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m    249\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 252\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    254\u001b[0m             clone(est),\n\u001b[1;32m    255\u001b[0m             X,\n\u001b[1;32m    256\u001b[0m             y,\n\u001b[1;32m    257\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[1;32m    258\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[1;32m    259\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    260\u001b[0m             fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    261\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     meth\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempt to pop from an empty stack"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "from catboost import CatBoostClassifier\n",
    "import contextlib\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model with a custom wrapper\n",
    "class CatBoostWrapper(CatBoostClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "            return super(CatBoostWrapper, self).fit(*args, **kwargs)\n",
    "\n",
    "cat = CatBoostWrapper(random_state=42, silent=True)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost with RandomizedSearchCV\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'depth': randint(3, 10),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 255),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Categorized 'medical_specialty' into meaningful groups.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest, Gradient Boosting, and CatBoost classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.635\n",
    "# - Precision: 0.383\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.653\n",
    "# - Precision: 0.435\n",
    "# - Recall: 0.033\n",
    "# - F1 Score: 0.062\n",
    "# - Balanced Accuracy: 0.514\n",
    "\n",
    "# CatBoost Performance:\n",
    "# - Accuracy: 0.885\n",
    "# - ROC-AUC: 0.661\n",
    "# - Precision: 0.485\n",
    "# - Recall: 0.029\n",
    "# - F1 Score: 0.054\n",
    "# - Balanced Accuracy: 0.512\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.880\n",
    "# - ROC-AUC: 0.655\n",
    "# - Precision: 0.379\n",
    "# - Recall: 0.080\n",
    "# - F1 Score: 0.132\n",
    "# - Balanced Accuracy: 0.531\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.659\n",
    "# - ROC-AUC: 0.655\n",
    "# - Precision: 0.179\n",
    "# - Recall: 0.548\n",
    "# - F1 Score: 0.269\n",
    "# - Balanced Accuracy: 0.611\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbdb0645-fb33-4d8b-abf7-e2f0e40fcdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for CatBoost: {'bagging_temperature': 0.7853406511139436, 'border_count': 135, 'depth': 6, 'iterations': 148, 'l2_leaf_reg': 4.722827665617431, 'learning_rate': 0.1980266884915557}\n",
      "Best ROC-AUC score for CatBoost: 0.9551220734583445\n",
      "CatBoost with Best Parameters:\n",
      "Accuracy: 0.8852332406549274\n",
      "ROC-AUC: 0.661459295552595\n",
      "Precision: 0.48484848484848486\n",
      "Recall: 0.028764044943820226\n",
      "F1: 0.054306321595248196\n",
      "Balanced Accuracy: 0.5124049334447542\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempt to pop from an empty stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 184, in log_fixup\n    yield\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2433, in _fit\n    self.get_feature_importance(type=EFstrType.PredictionValuesChange)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 3179, in get_feature_importance\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 986, in cross_val_predict\n    predictions = parallel(\n                  ^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1098, in __call__\n    self.retrieve()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 975, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 774, in get\n    raise self._value\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1068, in _fit_and_predict\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/var/folders/yd/8fns7t4j1n9gv77y0dvs5zdc0000gn/T/ipykernel_70231/2526550258.py\", line 204, in fit\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 5201, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2374, in _fit\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 250\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Fit the stacking model\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mdevnull, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 250\u001b[0m     stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Evaluate Stacking Model on the validation set\u001b[39;00m\n\u001b[1;32m    253\u001b[0m stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m    249\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 252\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    254\u001b[0m             clone(est),\n\u001b[1;32m    255\u001b[0m             X,\n\u001b[1;32m    256\u001b[0m             y,\n\u001b[1;32m    257\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[1;32m    258\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[1;32m    259\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    260\u001b[0m             fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    261\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     meth\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempt to pop from an empty stack"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "from catboost import CatBoostClassifier\n",
    "import contextlib\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model with a custom wrapper\n",
    "class CatBoostWrapper(CatBoostClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "            return super(CatBoostWrapper, self).fit(*args, **kwargs)\n",
    "\n",
    "cat = CatBoostWrapper(random_state=42, silent=True)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost with RandomizedSearchCV\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'depth': randint(3, 10),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 255),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Overview and Conclusion\n",
    "\n",
    "# The goal of this project was to build and evaluate a predictive model for patient readmission within 30 days using a diabetic dataset. The following steps were taken to achieve this:\n",
    "\n",
    "# Data Preprocessing:\n",
    "# - Loaded the dataset and handled missing values by replacing them with NaNs and dropping irrelevant columns.\n",
    "# - Converted 'age' feature from ranges to numerical averages.\n",
    "# - Categorized 'medical_specialty' into meaningful groups.\n",
    "# - Excluded rows where discharge_disposition_id indicates death or other non-readmission status.\n",
    "# - Conducted feature engineering to create new interaction features that may have predictive power.\n",
    "\n",
    "# Encoding and Feature Selection:\n",
    "# - Encoded categorical variables using one-hot encoding and transformed the target variable into a binary format.\n",
    "# - Further encoded remaining non-numeric columns to numeric codes.\n",
    "# - Standardized the dataset to ensure features are on the same scale.\n",
    "# - Used RFE for feature selection, identifying the most important features for the model.\n",
    "\n",
    "# Handling Class Imbalance:\n",
    "# - Addressed the class imbalance issue using a combination of SMOTE and RandomUnderSampler to resample the training data.\n",
    "\n",
    "# Model Building and Hyperparameter Tuning:\n",
    "# - Built Random Forest, Gradient Boosting, and CatBoost classifiers and performed hyperparameter tuning using RandomizedSearchCV to find the best parameters.\n",
    "# - Evaluated the models using various metrics, including accuracy, ROC-AUC, precision, recall, F1 score, and balanced accuracy.\n",
    "# - Combined models into a stacking classifier and optimized the classification threshold.\n",
    "\n",
    "# Insights and Evaluation\n",
    "\n",
    "# Random Forest Performance:\n",
    "# - Accuracy: 0.882\n",
    "# - ROC-AUC: 0.635\n",
    "# - Precision: 0.383\n",
    "# - Recall: 0.044\n",
    "# - F1 Score: 0.078\n",
    "# - Balanced Accuracy: 0.517\n",
    "\n",
    "# Gradient Boosting Performance:\n",
    "# - Accuracy: 0.884\n",
    "# - ROC-AUC: 0.653\n",
    "# - Precision: 0.435\n",
    "# - Recall: 0.033\n",
    "# - F1 Score: 0.062\n",
    "# - Balanced Accuracy: 0.514\n",
    "\n",
    "# CatBoost Performance:\n",
    "# - Accuracy: 0.885\n",
    "# - ROC-AUC: 0.661\n",
    "# - Precision: 0.485\n",
    "# - Recall: 0.029\n",
    "# - F1 Score: 0.054\n",
    "# - Balanced Accuracy: 0.512\n",
    "\n",
    "# Stacking Model Performance:\n",
    "# - Accuracy: 0.880\n",
    "# - ROC-AUC: 0.655\n",
    "# - Precision: 0.379\n",
    "# - Recall: 0.080\n",
    "# - F1 Score: 0.132\n",
    "# - Balanced Accuracy: 0.531\n",
    "\n",
    "# Final Stacking Model Performance with Best Threshold:\n",
    "# - Accuracy: 0.659\n",
    "# - ROC-AUC: 0.655\n",
    "# - Precision: 0.179\n",
    "# - Recall: 0.548\n",
    "# - F1 Score: 0.269\n",
    "# - Balanced Accuracy: 0.611\n",
    "\n",
    "# The model building and evaluation process revealed that combining models into a stacking classifier and optimizing the classification threshold can enhance predictive performance. Future work could involve exploring additional features, trying other algorithms, and validating the models on external datasets to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163161b7-9555-44fc-8c1b-ffca1858e80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 27, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 111}\n",
      "Best ROC-AUC score for Random Forest: 0.9598801925217965\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8824014004736896\n",
      "ROC-AUC: 0.6347570029525662\n",
      "Precision: 0.383399209486166\n",
      "Recall: 0.04359550561797753\n",
      "F1: 0.07828894269572235\n",
      "Balanced Accuracy: 0.517262077981984\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.09955663291461833, 'max_depth': 8, 'min_samples_leaf': 7, 'min_samples_split': 5, 'n_estimators': 104}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9547235776776032\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8843064565956132\n",
      "ROC-AUC: 0.6530379286170243\n",
      "Precision: 0.43529411764705883\n",
      "Recall: 0.03325842696629214\n",
      "F1: 0.0617954070981211\n",
      "Balanced Accuracy: 0.5138380289742202\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'bagging_temperature': 0.7853406511139436, 'border_count': 135, 'depth': 6, 'iterations': 148, 'l2_leaf_reg': 4.722827665617431, 'learning_rate': 0.1980266884915557}\n",
      "Best ROC-AUC score for CatBoost: 0.9551220734583445\n",
      "CatBoost with Best Parameters:\n",
      "Accuracy: 0.8852332406549274\n",
      "ROC-AUC: 0.661459295552595\n",
      "Precision: 0.48484848484848486\n",
      "Recall: 0.028764044943820226\n",
      "F1: 0.054306321595248196\n",
      "Balanced Accuracy: 0.5124049334447542\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempt to pop from an empty stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 986, in cross_val_predict\n    predictions = parallel(\n                  ^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1098, in __call__\n    self.retrieve()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 975, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 774, in get\n    raise self._value\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1068, in _fit_and_predict\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/var/folders/yd/8fns7t4j1n9gv77y0dvs5zdc0000gn/T/ipykernel_70231/1805750091.py\", line 204, in fit\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 5201, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2374, in _fit\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 249\u001b[0m\n\u001b[1;32m    238\u001b[0m stacking_model \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[1;32m    239\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    240\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, best_rf),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    246\u001b[0m )\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Fit the stacking model\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Evaluate Stacking Model on the validation set\u001b[39;00m\n\u001b[1;32m    252\u001b[0m stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m    249\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 252\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    254\u001b[0m             clone(est),\n\u001b[1;32m    255\u001b[0m             X,\n\u001b[1;32m    256\u001b[0m             y,\n\u001b[1;32m    257\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[1;32m    258\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[1;32m    259\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    260\u001b[0m             fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    261\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     meth\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempt to pop from an empty stack"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint, uniform\n",
    "from catboost import CatBoostClassifier\n",
    "import contextlib\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Define specialty categories\n",
    "high_frequency = ['InternalMedicine', 'Family/GeneralPractice', 'Cardiology', 'Surgery-General', 'Orthopedics', 'Orthopedics-Reconstructive', \n",
    "                 'Emergency/Trauma', 'Urology','ObstetricsandGynecology','Psychiatry','Pulmonology ','Nephrology','Radiologist']\n",
    "\n",
    "low_frequency = ['Surgery-PlasticwithinHeadandNeck','Psychiatry-Addictive','Proctology','Dermatology','SportsMedicine','Speech','Perinatology',\n",
    "                'Neurophysiology','Resident','Pediatrics-Hematology-Oncology','Pediatrics-EmergencyMedicine','Dentistry','DCPTEAM','Psychiatry-Child/Adolescent',\n",
    "                'Pediatrics-Pulmonology','Surgery-Pediatric','AllergyandImmunology','Pediatrics-Neurology','Anesthesiology','Pathology','Cardiology-Pediatric',\n",
    "                'Endocrinology-Metabolism','PhysicianNotFound','Surgery-Colon&Rectal','OutreachServices',\n",
    "                'Surgery-Maxillofacial','Rheumatology','Anesthesiology-Pediatric','Obstetrics','Obsterics&Gynecology-GynecologicOnco']\n",
    "\n",
    "pediatrics = ['Pediatrics','Pediatrics-CriticalCare','Pediatrics-EmergencyMedicine','Pediatrics-Endocrinology','Pediatrics-Hematology-Oncology',\n",
    "               'Pediatrics-Neurology','Pediatrics-Pulmonology', 'Anesthesiology-Pediatric', 'Cardiology-Pediatric', 'Surgery-Pediatric']\n",
    "\n",
    "psychic = ['Psychiatry-Addictive', 'Psychology', 'Psychiatry',  'Psychiatry-Child/Adolescent', 'PhysicalMedicineandRehabilitation', 'Osteopath']\n",
    "\n",
    "neurology = ['Neurology', 'Surgery-Neuro',  'Pediatrics-Neurology', 'Neurophysiology']\n",
    "\n",
    "surgery = ['Surgeon', 'Surgery-Cardiovascular', \n",
    "          'Surgery-Cardiovascular/Thoracic', 'Surgery-Colon&Rectal', 'Surgery-General', 'Surgery-Maxillofacial', \n",
    "             'Surgery-Plastic', 'Surgery-PlasticwithinHeadandNeck',  'Surgery-Thoracic',\n",
    "             'Surgery-Vascular', 'SurgicalSpecialty', 'Podiatry']\n",
    "             \n",
    "ungrouped = ['Endocrinology','Gastroenterology','Gynecology','Hematology','Hematology/Oncology','Hospitalist','InfectiousDiseases',\n",
    "           'Oncology','Ophthalmology','Otolaryngology','Pulmonology','Radiology']\n",
    "\n",
    "missing = ['?']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'patient_nbr', 'encounter_id'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Categorize 'medical_specialty'\n",
    "def categorize_specialty(specialty):\n",
    "    if pd.isna(specialty):\n",
    "        return 'missing'\n",
    "    elif specialty in high_frequency:\n",
    "        return 'high_frequency'\n",
    "    elif specialty in low_frequency:\n",
    "        return 'low_frequency'\n",
    "    elif specialty in pediatrics:\n",
    "        return 'pediatrics'\n",
    "    elif specialty in psychic:\n",
    "        return 'psychic'\n",
    "    elif specialty in neurology:\n",
    "        return 'neurology'\n",
    "    elif specialty in surgery:\n",
    "        return 'surgery'\n",
    "    elif specialty in ungrouped:\n",
    "        return 'ungrouped'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "data['medical_specialty'] = data['medical_specialty'].apply(categorize_specialty)\n",
    "\n",
    "# Exclude rows where discharge_disposition_id indicates death or other non-readmission status\n",
    "exclude_discharge_ids = [11, 13, 14, 19, 20, 21]\n",
    "data = data[~data['discharge_disposition_id'].isin(exclude_discharge_ids)]\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "data['num_lab_procedures_visits'] = data['num_lab_procedures'] * data['number_outpatient']\n",
    "data['num_medications_visits'] = data['num_medications'] * data['number_outpatient']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'medical_specialty']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with RFE\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=20, step=1)\n",
    "X_train_selected = rfe_selector.fit_transform(X_train_resampled, y_train_resampled)\n",
    "X_valid_selected = rfe_selector.transform(X_valid_scaled)\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest with RandomizedSearchCV\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(10, 30),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model with a custom wrapper\n",
    "class CatBoostWrapper(CatBoostClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "            return super(CatBoostWrapper, self).fit(*args, **kwargs)\n",
    "\n",
    "cat = CatBoostWrapper(random_state=42, silent=True)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost with RandomizedSearchCV\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'depth': randint(3, 10),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 255),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold Tuning for Stacking Model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.1\n",
    "best_f1 = stacking_f1\n",
    "\n",
    "for threshold in thresholds:\n",
    "    _, _, precision, recall, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Evaluate the final model with the best threshold\n",
    "final_accuracy, final_roc_auc, final_precision, final_recall, final_f1, final_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {final_accuracy}\")\n",
    "print(f\"ROC-AUC: {final_roc_auc}\")\n",
    "print(f\"Precision: {final_precision}\")\n",
    "print(f\"Recall: {final_recall}\")\n",
    "print(f\"F1: {final_f1}\")\n",
    "print(f\"Balanced Accuracy: {final_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae11b81-cf57-4584-8a11-7417ebf06d28",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'discharge_disposition_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'discharge_disposition_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadmitted\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadmitted\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<30\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Drop rows with specific discharge_disposition_id values\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m~\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdischarge_disposition_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m21\u001b[39m])]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Define features and target variable\u001b[39;00m\n\u001b[1;32m     39\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadmitted\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencounter_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_nbr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'discharge_disposition_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "import contextlib\n",
    "import os\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Drop rows with specific discharge_disposition_id values\n",
    "data = data[~data['discharge_disposition_id'].isin([11, 13, 14, 19, 20, 21])]\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = Pipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model with a custom wrapper\n",
    "class CatBoostWrapper(CatBoostClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "            return super(CatBoostWrapper, self).fit(*args, **kwargs)\n",
    "\n",
    "cat = CatBoostWrapper(random_state=42, silent=True)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost with RandomizedSearchCV\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'depth': randint(3, 10),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 255),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Find the best threshold for the stacking model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "for threshold in thresholds:\n",
    "    _, _, _, _, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final Stacking Model Evaluation with Best Threshold\n",
    "final_accuracy, final_roc_auc, final_precision, final_recall, final_f1, final_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {final_accuracy}\")\n",
    "print(f\"ROC-AUC: {final_roc_auc}\")\n",
    "print(f\"Precision: {final_precision}\")\n",
    "print(f\"Recall: {final_recall}\")\n",
    "print(f\"F1: {final_f1}\")\n",
    "print(f\"Balanced Accuracy: {final_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc84dd9d-1c01-472f-af33-5f10bf3cf94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'bootstrap': False, 'max_depth': 26, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 148}\n",
      "Best ROC-AUC score for Random Forest: 0.9749637380568336\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.884049016579137\n",
      "ROC-AUC: 0.6415749023379437\n",
      "Precision: 0.4028776978417266\n",
      "Recall: 0.025168539325842697\n",
      "F1: 0.047377326565143825\n",
      "Balanced Accuracy: 0.5101710580562457\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.07973319745834587, 'max_depth': 9, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 82}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9518452077608108\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8847698486252703\n",
      "ROC-AUC: 0.657562666077765\n",
      "Precision: 0.46524064171123\n",
      "Recall: 0.03910112359550562\n",
      "F1: 0.07213930348258707\n",
      "Balanced Accuracy: 0.5166430779342882\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'bagging_temperature': 0.3745401188473625, 'border_count': 124, 'depth': 9, 'iterations': 121, 'l2_leaf_reg': 6.986584841970366, 'learning_rate': 0.0412037280884873}\n",
      "Best ROC-AUC score for CatBoost: 0.948486527554271\n",
      "CatBoost with Best Parameters:\n",
      "Accuracy: 0.8856966326845845\n",
      "ROC-AUC: 0.6595011541730887\n",
      "Precision: 0.5242718446601942\n",
      "Recall: 0.024269662921348314\n",
      "F1: 0.04639175257731958\n",
      "Balanced Accuracy: 0.5107101643675765\n",
      "Stacking Model:\n",
      "Accuracy: 0.8729276078673669\n",
      "ROC-AUC: 0.6347434782523473\n",
      "Precision: 0.2879581151832461\n",
      "Recall: 0.07415730337078652\n",
      "F1: 0.11794138670478914\n",
      "Balanced Accuracy: 0.5252161175224579\n",
      "Best threshold: 0.1, Best F1 Score: 0.26091221186856306\n",
      "Final Stacking Model Evaluation with Best Threshold:\n",
      "Accuracy: 0.6896303161363402\n",
      "ROC-AUC: 0.6347434782523473\n",
      "Precision: 0.17939639183948744\n",
      "Recall: 0.47820224719101123\n",
      "F1: 0.26091221186856306\n",
      "Balanced Accuracy: 0.5975938839606856\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "import contextlib\n",
    "import os\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Drop rows with specific discharge_disposition_id values\n",
    "data = data[~data['discharge_disposition_id'].isin([11, 13, 14, 19, 20, 21])]\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = Pipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting with RandomizedSearchCV\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model with a custom wrapper\n",
    "class CatBoostWrapper(CatBoostClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "            return super(CatBoostWrapper, self).fit(*args, **kwargs)\n",
    "\n",
    "cat = CatBoostWrapper(random_state=42, silent=True)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost with RandomizedSearchCV\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 150),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'depth': randint(3, 10),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 255),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Find the best threshold for the stacking model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "for threshold in thresholds:\n",
    "    _, _, _, _, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final Stacking Model Evaluation with Best Threshold\n",
    "final_accuracy, final_roc_auc, final_precision, final_recall, final_f1, final_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {final_accuracy}\")\n",
    "print(f\"ROC-AUC: {final_roc_auc}\")\n",
    "print(f\"Precision: {final_precision}\")\n",
    "print(f\"Recall: {final_recall}\")\n",
    "print(f\"F1: {final_f1}\")\n",
    "print(f\"Balanced Accuracy: {final_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c6f8bb-2b8e-45d4-804f-c362765b1446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempt to pop from an empty stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 986, in cross_val_predict\n    predictions = parallel(\n                  ^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1098, in __call__\n    self.retrieve()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 975, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 774, in get\n    raise self._value\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1070, in _fit_and_predict\n    predictions = func(X_test)\n                  ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 5295, in predict_proba\n    return self._predict(X, 'Probability', ntree_start, ntree_end, thread_count, verbose, 'predict_proba', task_type)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2598, in _predict\n    data, data_is_single_object = self._process_predict_input_data(data, parent_method_name, thread_count)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2578, in _process_predict_input_data\n    data = Pool(\n           ^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 757, in __init__\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 120\u001b[0m\n\u001b[1;32m    108\u001b[0m stacking_model \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[1;32m    109\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    110\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, best_rf),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    117\u001b[0m )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Fit the stacking model\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train_resampled, y_train_resampled)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Model evaluation function with threshold adjustment\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, X_valid, y_valid, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m    249\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 252\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    254\u001b[0m             clone(est),\n\u001b[1;32m    255\u001b[0m             X,\n\u001b[1;32m    256\u001b[0m             y,\n\u001b[1;32m    257\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[1;32m    258\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[1;32m    259\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    260\u001b[0m             fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    261\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     meth\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempt to pop from an empty stack"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score, confusion_matrix\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert age ranges to numerical values\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Drop rows with specific discharge_disposition_id values\n",
    "data = data[~data['discharge_disposition_id_11'].isin([11, 13, 14, 19, 20, 21])]\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "                                      param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(GradientBoostingClassifier(random_state=42),\n",
    "                                      param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for CatBoost\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 200),\n",
    "    'depth': randint(4, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 150),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(CatBoostClassifier(random_state=42, silent=True),\n",
    "                                       param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_cat.fit(X_train_resampled, y_train_resampled)\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Model evaluation function with threshold adjustment\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_proba)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Find the best threshold for the stacking model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "for threshold in thresholds:\n",
    "    _, _, _, _, f1, _ = evaluate_model(stacking_model, X_valid_scaled, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final Stacking Model Evaluation with Best Threshold\n",
    "final_accuracy, final_roc_auc, final_precision, final_recall, final_f1, final_balanced_accuracy = evaluate_model(stacking_model, X_valid_scaled, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {final_accuracy}\")\n",
    "print(f\"ROC-AUC: {final_roc_auc}\")\n",
    "print(f\"Precision: {final_precision}\")\n",
    "print(f\"Recall: {final_recall}\")\n",
    "print(f\"F1: {final_f1}\")\n",
    "print(f\"Balanced Accuracy: {final_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b86f71-ae3b-4a0b-a38b-f6791c82bcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempt to pop from an empty stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 986, in cross_val_predict\n    predictions = parallel(\n                  ^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1098, in __call__\n    self.retrieve()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 975, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 774, in get\n    raise self._value\n  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1070, in _fit_and_predict\n    predictions = func(X_test)\n                  ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 5295, in predict_proba\n    return self._predict(X, 'Probability', ntree_start, ntree_end, thread_count, verbose, 'predict_proba', task_type)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2598, in _predict\n    data, data_is_single_object = self._process_predict_input_data(data, parent_method_name, thread_count)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2578, in _process_predict_input_data\n    data = Pool(\n           ^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 757, in __init__\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 123\u001b[0m\n\u001b[1;32m    111\u001b[0m stacking_model \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[1;32m    112\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    113\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, best_rf),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Fit the stacking model\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train_resampled, y_train_resampled)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Model evaluation function with threshold adjustment\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(model, X_valid, y_valid, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:660\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m    249\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 252\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    254\u001b[0m             clone(est),\n\u001b[1;32m    255\u001b[0m             X,\n\u001b[1;32m    256\u001b[0m             y,\n\u001b[1;32m    257\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[1;32m    258\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[1;32m    259\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    260\u001b[0m             fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    261\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    262\u001b[0m         )\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    270\u001b[0m     meth\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempt to pop from an empty stack"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert age ranges to numerical values\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Drop rows with specific discharge_disposition_id values\n",
    "data = data[~data['discharge_disposition_id_11'].isin([11, 13, 14, 19, 20, 21])]\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "                                      param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(GradientBoostingClassifier(random_state=42),\n",
    "                                      param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for CatBoost\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 200),\n",
    "    'depth': randint(4, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 150),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(CatBoostClassifier(random_state=42, silent=True),\n",
    "                                       param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_cat.fit(X_train_resampled, y_train_resampled)\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "\n",
    "# Ensure CatBoost doesn't log during parallel processes\n",
    "def fit_with_no_logging(model, X, y):\n",
    "    pool = Pool(X, y)\n",
    "    return model.fit(pool, verbose=False)\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Model evaluation function with threshold adjustment\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_proba)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Find the best threshold for the stacking model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "for threshold in thresholds:\n",
    "    _, _, _, _, f1, _ = evaluate_model(stacking_model, X_valid_scaled, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final Stacking Model Evaluation with Best Threshold\n",
    "final_accuracy, final_roc_auc, final_precision, final_recall, final_f1, final_balanced_accuracy = evaluate_model(stacking_model, X_valid_scaled, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {final_accuracy}\")\n",
    "print(f\"ROC-AUC: {final_roc_auc}\")\n",
    "print(f\"Precision: {final_precision}\")\n",
    "print(f\"Recall: {final_recall}\")\n",
    "print(f\"F1: {final_f1}\")\n",
    "print(f\"Balanced Accuracy: {final_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814b8ae7-a2e4-46a1-9bf4-31518ef3529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'bagging_temperature' for estimator CatBoostWrapper(). Valid parameters are: [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 428, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 674, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py\", line 205, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'bagging_temperature' for estimator CatBoostWrapper(). Valid parameters are: [].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 124\u001b[0m\n\u001b[1;32m    113\u001b[0m param_dist_cat \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m200\u001b[39m),\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbagging_temperature\u001b[39m\u001b[38;5;124m'\u001b[39m: uniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    120\u001b[0m }\n\u001b[1;32m    122\u001b[0m random_search_cat \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(CatBoostWrapper(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m    123\u001b[0m                                        param_distributions\u001b[38;5;241m=\u001b[39mparam_dist_cat, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m random_search_cat\u001b[38;5;241m.\u001b[39mfit(X_train_resampled, y_train_resampled)\n\u001b[1;32m    125\u001b[0m best_cat \u001b[38;5;241m=\u001b[39m random_search_cat\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Stacking Classifier with meta-classifier\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter 'bagging_temperature' for estimator CatBoostWrapper(). Valid parameters are: []."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "# Custom CatBoost wrapper to handle logging\n",
    "class CatBoostWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, **params):\n",
    "        self.model = CatBoostClassifier(**params, verbose=0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pool = Pool(X, y)\n",
    "        self.model.fit(pool)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert age ranges to numerical values\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Drop rows with specific discharge_disposition_id values\n",
    "data = data[~data['discharge_disposition_id_11'].isin([11, 13, 14, 19, 20, 21])]\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "                                      param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(GradientBoostingClassifier(random_state=42),\n",
    "                                      param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for CatBoost\n",
    "param_dist_cat = {\n",
    "    'iterations': randint(50, 200),\n",
    "    'depth': randint(4, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'border_count': randint(32, 150),\n",
    "    'bagging_temperature': uniform(0, 1)\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(CatBoostWrapper(random_state=42),\n",
    "                                       param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_cat.fit(X_train_resampled, y_train_resampled)\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Model evaluation function with threshold adjustment\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_proba)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Find the best threshold for the stacking model\n",
    "thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "for threshold in thresholds:\n",
    "    _, _, _, _, f1, _ = evaluate_model(stacking_model, X_valid_scaled, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final Stacking Model Evaluation with Best Threshold\n",
    "final_accuracy, final_roc_auc, final_precision, final_recall, final_f1, final_balanced_accuracy = evaluate_model(stacking_model, X_valid_scaled, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {final_accuracy}\")\n",
    "print(f\"ROC-AUC: {final_roc_auc}\")\n",
    "print(f\"Precision: {final_precision}\")\n",
    "print(f\"Recall: {final_recall}\")\n",
    "print(f\"F1: {final_f1}\")\n",
    "print(f\"Balanced Accuracy: {final_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adf55f4-1967-424f-a6ae-cc9a8ba08551",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute '_check_fit_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m\n\u001b[1;32m     55\u001b[0m resampling_pipeline \u001b[38;5;241m=\u001b[39m ImbPipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     56\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmotetomek\u001b[39m\u001b[38;5;124m'\u001b[39m, SMOTETomek(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[1;32m     57\u001b[0m ])\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Apply the pipeline to the training data\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m X_train_resampled, y_train_resampled \u001b[38;5;241m=\u001b[39m resampling_pipeline\u001b[38;5;241m.\u001b[39mfit_resample(X_train_scaled, y_train)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Extended Hyperparameter tuning for Random Forest\u001b[39;00m\n\u001b[1;32m     63\u001b[0m param_dist_rf \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m200\u001b[39m),\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[1;32m     70\u001b[0m }\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/imblearn/pipeline.py:372\u001b[0m, in \u001b[0;36mPipeline.fit_resample\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and sample with the final estimator.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03mFits all the transformers/samplers one after the other and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    Transformed target.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 372\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    373\u001b[0m Xt, yt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[1;32m    374\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute '_check_fit_params'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert age ranges to numerical values\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Drop rows with specific discharge_disposition_id values\n",
    "data = data[~data['discharge_disposition_id_11'].isin([11, 13, 14, 19, 20, 21])]\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Pipeline for SMOTETomek\n",
    "resampling_pipeline = ImbPipeline(steps=[\n",
    "    ('smotetomek', SMOTETomek(random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Extended Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "                                      param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [best_rf.get_params()['n_estimators'] - 20, best_rf.get_params()['n_estimators'], best_rf.get_params()['n_estimators'] + 20],\n",
    "    'max_features': [best_rf.get_params()['max_features']],\n",
    "    'max_depth': [best_rf.get_params()['max_depth'] - 2, best_rf.get_params()['max_depth'], best_rf.get_params()['max_depth'] + 2],\n",
    "    'min_samples_split': [best_rf.get_params()['min_samples_split'] - 1, best_rf.get_params()['min_samples_split'], best_rf.get_params()['min_samples_split'] + 1],\n",
    "    'min_samples_leaf': [best_rf.get_params()['min_samples_leaf'] - 1, best_rf.get_params()['min_samples_leaf'], best_rf.get_params()['min_samples_leaf'] + 1],\n",
    "    'bootstrap': [best_rf.get_params()['bootstrap']]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "                              param_grid=param_grid_rf, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "best_rf_grid = grid_search_rf.best_estimator_\n",
    "\n",
    "# Extended Hyperparameter tuning for Gradient Boosting\n",
    "param_dist_gb = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20)\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(GradientBoostingClassifier(random_state=42),\n",
    "                                      param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [best_gb.get_params()['n_estimators'] - 20, best_gb.get_params()['n_estimators'], best_gb.get_params()['n_estimators'] + 20],\n",
    "    'max_depth': [best_gb.get_params()['max_depth'] - 2, best_gb.get_params()['max_depth'], best_gb.get_params()['max_depth'] + 2],\n",
    "    'learning_rate': [best_gb.get_params()['learning_rate'] - 0.02, best_gb.get_params()['learning_rate'], best_gb.get_params()['learning_rate'] + 0.02],\n",
    "    'min_samples_split': [best_gb.get_params()['min_samples_split'] - 1, best_gb.get_params()['min_samples_split'], best_gb.get_params()['min_samples_split'] + 1],\n",
    "    'min_samples_leaf': [best_gb.get_params()['min_samples_leaf'] - 1, best_gb.get_params()['min_samples_leaf'], best_gb.get_params()['min_samples_leaf'] + 1]\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(GradientBoostingClassifier(random_state=42),\n",
    "                              param_grid=param_grid_gb, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid_search_gb.fit(X_train_resampled, y_train_resampled)\n",
    "best_gb_grid = grid_search_gb.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for CatBoost\n",
    "param_dist_cb = {\n",
    "    'iterations': randint(50, 200),\n",
    "    'depth': randint(3, 12),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'l2_leaf_reg': uniform(1, 10),\n",
    "    'bagging_temperature': uniform(0, 1),\n",
    "    'border_count': randint(1, 255)\n",
    "}\n",
    "\n",
    "random_search_cb = RandomizedSearchCV(CatBoostClassifier(verbose=0, random_state=42),\n",
    "                                      param_distributions=param_dist_cb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_cb.fit(X_train_resampled, y_train_resampled)\n",
    "best_cb = random_search_cb.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for LightGBM\n",
    "param_dist_lgb = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'num_leaves': randint(20, 150),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'min_child_samples': randint(5, 100),\n",
    "    'subsample': uniform(0.5, 1),\n",
    "    'colsample_bytree': uniform(0.5, 1)\n",
    "}\n",
    "\n",
    "random_search_lgb = RandomizedSearchCV(LGBMClassifier(random_state=42, class_weight='balanced'),\n",
    "                                       param_distributions=param_dist_lgb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_lgb.fit(X_train_resampled, y_train_resampled)\n",
    "best_lgb = random_search_lgb.best_estimator_\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf_grid),\n",
    "        ('gb', best_gb_grid),\n",
    "        ('cb', best_cb),\n",
    "        ('lgb', best_lgb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "stacking_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Model evaluation function with threshold adjustment\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_pred_proba = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_pred_proba)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_scaled, y_valid)\n",
    "\n",
    "# Print the evaluation metrics for the stacking model\n",
    "print(f\"Stacking Model:\\n\"\n",
    "      f\"Accuracy: {stacking_accuracy}\\n\"\n",
    "      f\"ROC-AUC: {stacking_roc_auc}\\n\"\n",
    "      f\"Precision: {stacking_precision}\\n\"\n",
    "      f\"Recall: {stacking_recall}\\n\"\n",
    "      f\"F1: {stacking_f1}\\n\"\n",
    "      f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Finding the best threshold for the stacking model\n",
    "best_threshold = 0.1  # You can tune this based on ROC curve or other criteria\n",
    "best_f1_score = 0.26091221186856306  # Update this after tuning\n",
    "\n",
    "# Final Stacking Model Evaluation with Best Threshold\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_scaled, y_valid, threshold=best_threshold)\n",
    "\n",
    "# Print the evaluation metrics for the stacking model with the best threshold\n",
    "print(f\"Final Stacking Model Evaluation with Best Threshold:\\n\"\n",
    "      f\"Accuracy: {stacking_accuracy}\\n\"\n",
    "      f\"ROC-AUC: {stacking_roc_auc}\\n\"\n",
    "      f\"Precision: {stacking_precision}\\n\"\n",
    "      f\"Recall: {stacking_recall}\\n\"\n",
    "      f\"F1: {stacking_f1}\\n\"\n",
    "      f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b04d2b-799a-4d9c-8c45-9e90c0ff3193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a41018-0a37-418a-a2d0-5c4ae2b21e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'n_estimators': 150, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30, 'bootstrap': False}\n",
      "Best ROC-AUC score for Random Forest: 0.9837353605871594\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8864767073722297\n",
      "ROC-AUC: 0.6416505104322692\n",
      "Precision: 0.39669421487603307\n",
      "Recall: 0.021486123545210387\n",
      "F1: 0.04076433121019108\n",
      "Balanced Accuracy: 0.5086768291091464\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'n_estimators': 94, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 9, 'learning_rate': 0.1}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9524422195685772\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8873812754409769\n",
      "ROC-AUC: 0.6709184993466133\n",
      "Precision: 0.4774193548387097\n",
      "Recall: 0.03312444046553268\n",
      "F1: 0.06195060694851402\n",
      "Balanced Accuracy: 0.5142695511130381\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for CatBoost: {'learning_rate': 0.05, 'l2_leaf_reg': 3, 'iterations': 83, 'depth': 10, 'border_count': 64, 'bagging_temperature': 1.0}\n",
      "Best ROC-AUC score for CatBoost: 0.9509980717940024\n",
      "CatBoost with Best Parameters:\n",
      "Accuracy: 0.8875822905673652\n",
      "ROC-AUC: 0.6641230743968938\n",
      "Precision: 0.46511627906976744\n",
      "Recall: 0.008952551477170993\n",
      "F1: 0.01756697408871322\n",
      "Balanced Accuracy: 0.5038252709268107\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030380 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042344 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.069585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.081583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044659 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.073081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037572 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130307 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104466 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034672 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 70578, number of negative: 70578\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013365 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9338\n",
      "[LightGBM] [Info] Number of data points in the train set: 141156, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best parameters for LightGBM: {'subsample': 1.0, 'num_leaves': 127, 'n_estimators': 105, 'min_child_samples': 10, 'max_depth': 9, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
      "Best ROC-AUC score for LightGBM: 0.9525089768538697\n",
      "LightGBM with Best Parameters:\n",
      "Accuracy: 0.886677722498618\n",
      "ROC-AUC: 0.6656815988197735\n",
      "Precision: 0.45248868778280543\n",
      "Recall: 0.044762757385854966\n",
      "F1: 0.0814663951120163\n",
      "Balanced Accuracy: 0.5189565272918519\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 70578, number of negative: 70578\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9338\n",
      "[LightGBM] [Info] Number of data points in the train set: 141156, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56462\n",
      "[LightGBM] [Info] Number of positive: 56463, number of negative: 56462\n",
      "[LightGBM] [Info] Number of positive: 56463, number of negative: 56462\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56463\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56463\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9293\n",
      "[LightGBM] [Info] Number of data points in the train set: 112924, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.470282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9039\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000018\n",
      "[LightGBM] [Info] Start training from score 0.000018\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.482331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9045\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 77\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499996 -> initscore=-0.000018\n",
      "[LightGBM] [Info] Start training from score -0.000018\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.653665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9052\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499996 -> initscore=-0.000018\n",
      "[LightGBM] [Info] Start training from score -0.000018\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.665616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9044\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 79\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000018\n",
      "[LightGBM] [Info] Start training from score 0.000018\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempt to pop from an empty stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\njoblib.externals.loky.process_executor._RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 184, in log_fixup\n    yield\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2433, in _fit\n    self.get_feature_importance(type=EFstrType.PredictionValuesChange)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 3179, in get_feature_importance\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_utils.py\", line 72, in __call__\n    return self.func(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1378, in _fit_and_predict\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 5201, in fit\n    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2374, in _fit\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 155, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1293, in cross_val_predict\n    predictions = parallel(\n                  ^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 67, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 2007, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1650, in _get_outputs\n    yield from self._retrieve()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1754, in _retrieve\n    self._raise_error_fast()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1789, in _raise_error_fast\n    error_job.get_result(self.timeout)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 745, in get_result\n    return self._return_or_raise()\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 763, in _return_or_raise\n    raise self._result\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 222\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Fit the stacking model\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mdevnull, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 222\u001b[0m     stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Evaluate Stacking Model on the validation set\u001b[39;00m\n\u001b[1;32m    225\u001b[0m stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:669\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    668\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:259\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    254\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m    256\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    257\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     )\n\u001b[0;32m--> 259\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    260\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    261\u001b[0m             clone(est),\n\u001b[1;32m    262\u001b[0m             X,\n\u001b[1;32m    263\u001b[0m             y,\n\u001b[1;32m    264\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[1;32m    265\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[1;32m    266\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    267\u001b[0m             params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    268\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    269\u001b[0m         )\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    277\u001b[0m     meth\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempt to pop from an empty stack"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import contextlib\n",
    "import os\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Drop rows with specific discharge_disposition_id values\n",
    "data = data[~data['discharge_disposition_id_11'].isin([11, 13, 14, 19, 20, 21])]\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Apply SMOTETomek to the training data\n",
    "smotetomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smotetomek.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [int(x) for x in np.linspace(5, 30, num=6)],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "param_dist_gb = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model\n",
    "cat = cb.CatBoostClassifier(random_state=42, verbose=0)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost\n",
    "param_dist_cat = {\n",
    "    'iterations': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'border_count': [32, 64, 128],\n",
    "    'bagging_temperature': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Initialize LightGBM model\n",
    "lgbm = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for LightGBM\n",
    "param_dist_lgb = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'num_leaves': [31, 63, 127],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "random_search_lgb = RandomizedSearchCV(lgbm, param_distributions=param_dist_lgb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_lgb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best LightGBM model\n",
    "best_lgb = random_search_lgb.best_estimator_\n",
    "print(\"Best parameters for LightGBM:\", random_search_lgb.best_params_)\n",
    "print(\"Best ROC-AUC score for LightGBM:\", random_search_lgb.best_score_)\n",
    "\n",
    "# Evaluate LightGBM on the validation set\n",
    "lgb_accuracy, lgb_roc_auc, lgb_precision, lgb_recall, lgb_f1, lgb_balanced_accuracy = evaluate_model(best_lgb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"LightGBM with Best Parameters:\")\n",
    "print(f\"Accuracy: {lgb_accuracy}\")\n",
    "print(f\"ROC-AUC: {lgb_roc_auc}\")\n",
    "print(f\"Precision: {lgb_precision}\")\n",
    "print(f\"Recall: {lgb_recall}\")\n",
    "print(f\"F1: {lgb_f1}\")\n",
    "print(f\"Balanced Accuracy: {lgb_balanced_accuracy}\")\n",
    "\n",
    "# Stacking Classifier with meta-classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('gb', best_gb),\n",
    "        ('cat', best_cat),\n",
    "        ('lgb', best_lgb)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n",
    "\n",
    "# Threshold optimization for the stacking model\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "for threshold in np.arange(0.1, 0.9, 0.1):\n",
    "    _, _, _, _, f1, _ = evaluate_model(stacking_model, X_valid_selected, y_valid, threshold)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Final evaluation with the best threshold\n",
    "final_stacking_accuracy, final_stacking_roc_auc, final_stacking_precision, final_stacking_recall, final_stacking_f1, final_stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid, best_threshold)\n",
    "\n",
    "print(\"Final Stacking Model Evaluation with Best Threshold:\")\n",
    "print(f\"Accuracy: {final_stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {final_stacking_roc_auc}\")\n",
    "print(f\"Precision: {final_stacking_precision}\")\n",
    "print(f\"Recall: {final_stacking_recall}\")\n",
    "print(f\"F1: {final_stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {final_stacking_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee6343f-1e0b-4db3-9e47-61932cd29611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'n_estimators': 150, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30, 'bootstrap': False}\n",
      "Best ROC-AUC score for Random Forest: 0.9837353605871594\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8864767073722297\n",
      "ROC-AUC: 0.6416505104322692\n",
      "Precision: 0.39669421487603307\n",
      "Recall: 0.021486123545210387\n",
      "F1: 0.04076433121019108\n",
      "Balanced Accuracy: 0.5086768291091464\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'n_estimators': 94, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 9, 'learning_rate': 0.1}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9524422195685772\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8873812754409769\n",
      "ROC-AUC: 0.6709184993466133\n",
      "Precision: 0.4774193548387097\n",
      "Recall: 0.03312444046553268\n",
      "F1: 0.06195060694851402\n",
      "Balanced Accuracy: 0.5142695511130381\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.05, 'l2_leaf_reg': 3, 'iterations': 83, 'depth': 10, 'border_count': 64, 'bagging_temperature': 1.0}\n",
      "Best ROC-AUC score for CatBoost: 0.9509980717940024\n",
      "CatBoost with Best Parameters:\n",
      "Accuracy: 0.8875822905673652\n",
      "ROC-AUC: 0.6641230743968938\n",
      "Precision: 0.46511627906976744\n",
      "Recall: 0.008952551477170993\n",
      "F1: 0.01756697408871322\n",
      "Balanced Accuracy: 0.5038252709268107\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077852 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.127365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 70578, number of negative: 70578\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9314\n",
      "[LightGBM] [Info] Number of data points in the train set: 141156, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best parameters for LightGBM: {'subsample': 0.8, 'num_leaves': 127, 'n_estimators': 150, 'min_child_samples': 20, 'max_depth': -1, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "Best ROC-AUC score for LightGBM: 0.9536213262555591\n",
      "LightGBM with Best Parameters:\n",
      "Accuracy: 0.8875822905673652\n",
      "ROC-AUC: 0.6690722161505245\n",
      "Precision: 0.48951048951048953\n",
      "Recall: 0.03133393017009848\n",
      "F1: 0.058897770298695834\n",
      "Balanced Accuracy: 0.5136007324215904\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112590 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 70578, number of negative: 70578\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9314\n",
      "[LightGBM] [Info] Number of data points in the train set: 141156, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 56463, number of negative: 56462\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56463\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56463\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56462\n",
      "[LightGBM] [Info] Number of positive: 56463, number of negative: 56462\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.786200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9020\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 73\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.528330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9274\n",
      "[LightGBM] [Info] Number of data points in the train set: 112924, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000018\n",
      "[LightGBM] [Info] Start training from score 0.000018\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.880148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9027\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499996 -> initscore=-0.000018\n",
      "[LightGBM] [Info] Start training from score -0.000018\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.923811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9015\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000018\n",
      "[LightGBM] [Info] Start training from score 0.000018\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.951303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9024\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499996 -> initscore=-0.000018\n",
      "[LightGBM] [Info] Start training from score -0.000018\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempt to pop from an empty stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\njoblib.externals.loky.process_executor._RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/_utils.py\", line 72, in __call__\n    return self.func(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1380, in _fit_and_predict\n    predictions = func(X_test)\n                  ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 5295, in predict_proba\n    return self._predict(X, 'Probability', ntree_start, ntree_end, thread_count, verbose, 'predict_proba', task_type)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2598, in _predict\n    data, data_is_single_object = self._process_predict_input_data(data, parent_method_name, thread_count)\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 2578, in _process_predict_input_data\n    data = Pool(\n           ^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 757, in __init__\n    with log_fixup(log_cout, log_cerr):\n  File \"/opt/anaconda3/lib/python3.11/contextlib.py\", line 144, in __exit__\n    next(self.gen)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 186, in log_fixup\n    _custom_loggers_stack.pop()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/catboost/core.py\", line 165, in pop\n    raise RuntimeError('Attempt to pop from an empty stack')\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 1293, in cross_val_predict\n    predictions = parallel(\n                  ^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 67, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 2007, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1650, in _get_outputs\n    yield from self._retrieve()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1754, in _retrieve\n    self._raise_error_fast()\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1789, in _raise_error_fast\n    error_job.get_result(self.timeout)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 745, in get_result\n    return self._return_or_raise()\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 763, in _return_or_raise\n    raise self._result\nRuntimeError: Attempt to pop from an empty stack\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 225\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# Fit the stacking model\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mdevnull, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m--> 225\u001b[0m     stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train_selected, y_train_resampled)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Evaluate Stacking Model on the validation set\u001b[39;00m\n\u001b[1;32m    228\u001b[0m stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:669\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m    668\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m--> 669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:259\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    254\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m    256\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    257\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     )\n\u001b[0;32m--> 259\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    260\u001b[0m         delayed(cross_val_predict)(\n\u001b[1;32m    261\u001b[0m             clone(est),\n\u001b[1;32m    262\u001b[0m             X,\n\u001b[1;32m    263\u001b[0m             y,\n\u001b[1;32m    264\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[1;32m    265\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[1;32m    266\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    267\u001b[0m             params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m    268\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    269\u001b[0m         )\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    277\u001b[0m     meth\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempt to pop from an empty stack"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "import catboost as cb\n",
    "import contextlib\n",
    "import os\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Drop rows with specific discharge_disposition_id values\n",
    "data = data[~data['discharge_disposition_id_11'].isin([11, 13, 14, 19, 20, 21])]\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Apply SMOTETomek to the training data\n",
    "smotetomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smotetomek.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [int(x) for x in np.linspace(5, 30, num=6)],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "param_dist_gb = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize CatBoost model\n",
    "cat = cb.CatBoostClassifier(random_state=42, verbose=0)\n",
    "\n",
    "# Hyperparameter tuning for CatBoost\n",
    "param_dist_cat = {\n",
    "    'iterations': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'border_count': [32, 64, 128],\n",
    "    'bagging_temperature': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "random_search_cat = RandomizedSearchCV(cat, param_distributions=param_dist_cat, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_cat.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best CatBoost model\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "print(\"Best parameters for CatBoost:\", random_search_cat.best_params_)\n",
    "print(\"Best ROC-AUC score for CatBoost:\", random_search_cat.best_score_)\n",
    "\n",
    "# Evaluate CatBoost on the validation set\n",
    "cat_accuracy, cat_roc_auc, cat_precision, cat_recall, cat_f1, cat_balanced_accuracy = evaluate_model(best_cat, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"CatBoost with Best Parameters:\")\n",
    "print(f\"Accuracy: {cat_accuracy}\")\n",
    "print(f\"ROC-AUC: {cat_roc_auc}\")\n",
    "print(f\"Precision: {cat_precision}\")\n",
    "print(f\"Recall: {cat_recall}\")\n",
    "print(f\"F1: {cat_f1}\")\n",
    "print(f\"Balanced Accuracy: {cat_balanced_accuracy}\")\n",
    "\n",
    "# Initialize LightGBM model\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for LightGBM\n",
    "param_dist_lgbm = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 62, 127],\n",
    "    'max_depth': [-1, 5, 9, 13],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "random_search_lgbm = RandomizedSearchCV(lgbm, param_distributions=param_dist_lgbm, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_lgbm.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best LightGBM model\n",
    "best_lgbm = random_search_lgbm.best_estimator_\n",
    "print(\"Best parameters for LightGBM:\", random_search_lgbm.best_params_)\n",
    "print(\"Best ROC-AUC score for LightGBM:\", random_search_lgbm.best_score_)\n",
    "\n",
    "# Evaluate LightGBM on the validation set\n",
    "lgbm_accuracy, lgbm_roc_auc, lgbm_precision, lgbm_recall, lgbm_f1, lgbm_balanced_accuracy = evaluate_model(best_lgbm, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"LightGBM with Best Parameters:\")\n",
    "print(f\"Accuracy: {lgbm_accuracy}\")\n",
    "print(f\"ROC-AUC: {lgbm_roc_auc}\")\n",
    "print(f\"Precision: {lgbm_precision}\")\n",
    "print(f\"Recall: {lgbm_recall}\")\n",
    "print(f\"F1: {lgbm_f1}\")\n",
    "print(f\"Balanced Accuracy: {lgbm_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Stacking model\n",
    "estimators = [\n",
    "    ('rf', best_rf),\n",
    "    ('gb', best_gb),\n",
    "    ('cat', best_cat),\n",
    "    ('lgbm', best_lgbm)\n",
    "]\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ae407a-8d04-435f-a0d9-77be2cc7fc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'n_estimators': 150, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30, 'bootstrap': False}\n",
      "Best ROC-AUC score for Random Forest: 0.9837353605871594\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8864767073722297\n",
      "ROC-AUC: 0.6416505104322692\n",
      "Precision: 0.39669421487603307\n",
      "Recall: 0.021486123545210387\n",
      "F1: 0.04076433121019108\n",
      "Balanced Accuracy: 0.5086768291091464\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'n_estimators': 94, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 9, 'learning_rate': 0.1}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9524422195685772\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8873812754409769\n",
      "ROC-AUC: 0.6709184993466133\n",
      "Precision: 0.4774193548387097\n",
      "Recall: 0.03312444046553268\n",
      "F1: 0.06195060694851402\n",
      "Balanced Accuracy: 0.5142695511130381\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 70578, number of negative: 70578\n",
      "[LightGBM] [Info] Total Bins 9314\n",
      "[LightGBM] [Info] Number of data points in the train set: 141156, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best parameters for LightGBM: {'subsample': 0.8, 'num_leaves': 127, 'n_estimators': 150, 'min_child_samples': 20, 'max_depth': -1, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "Best ROC-AUC score for LightGBM: 0.9536213262555591\n",
      "LightGBM with Best Parameters:\n",
      "Accuracy: 0.8875822905673652\n",
      "ROC-AUC: 0.6690722161505245\n",
      "Precision: 0.48951048951048953\n",
      "Recall: 0.03133393017009848\n",
      "F1: 0.058897770298695834\n",
      "Balanced Accuracy: 0.5136007324215904\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 70578, number of negative: 70578\n",
      "[LightGBM] [Info] Total Bins 9314\n",
      "[LightGBM] [Info] Number of data points in the train set: 141156, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56463\n",
      "[LightGBM] [Info] Number of positive: 56463, number of negative: 56462\n",
      "[LightGBM] [Info] Number of positive: 56463, number of negative: 56462\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56462\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56463\n",
      "[LightGBM] [Info] Total Bins 9027\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499996 -> initscore=-0.000018\n",
      "[LightGBM] [Info] Start training from score -0.000018\n",
      "[LightGBM] [Info] Total Bins 9015\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 73\n",
      "[LightGBM] [Info] Total Bins 9020\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000018\n",
      "[LightGBM] [Info] Start training from score 0.000018\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000018\n",
      "[LightGBM] [Info] Start training from score 0.000018\n",
      "[LightGBM] [Info] Total Bins 9024\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499996 -> initscore=-0.000018\n",
      "[LightGBM] [Info] Start training from score -0.000018\n",
      "[LightGBM] [Info] Total Bins 9274\n",
      "[LightGBM] [Info] Number of data points in the train set: 112924, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Stacking Model:\n",
      "Accuracy: 0.8756721443288608\n",
      "ROC-AUC: 0.6285676601811137\n",
      "Precision: 0.28647686832740216\n",
      "Recall: 0.0720680393912265\n",
      "F1: 0.11516452074391989\n",
      "Balanced Accuracy: 0.5246838923251066\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from lightgbm import LGBMClassifier\n",
    "import contextlib\n",
    "import os\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Drop rows with specific discharge_disposition_id values\n",
    "data = data[~data['discharge_disposition_id_11'].isin([11, 13, 14, 19, 20, 21])]\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Apply SMOTETomek to the training data\n",
    "smotetomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smotetomek.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [int(x) for x in np.linspace(5, 30, num=6)],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "param_dist_gb = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize LightGBM model\n",
    "lgbm = LGBMClassifier(random_state=42, force_row_wise=True)\n",
    "\n",
    "# Hyperparameter tuning for LightGBM\n",
    "param_dist_lgbm = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 62, 127],\n",
    "    'max_depth': [-1, 5, 9, 13],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "random_search_lgbm = RandomizedSearchCV(lgbm, param_distributions=param_dist_lgbm, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_lgbm.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best LightGBM model\n",
    "best_lgbm = random_search_lgbm.best_estimator_\n",
    "print(\"Best parameters for LightGBM:\", random_search_lgbm.best_params_)\n",
    "print(\"Best ROC-AUC score for LightGBM:\", random_search_lgbm.best_score_)\n",
    "\n",
    "# Evaluate LightGBM on the validation set\n",
    "lgbm_accuracy, lgbm_roc_auc, lgbm_precision, lgbm_recall, lgbm_f1, lgbm_balanced_accuracy = evaluate_model(best_lgbm, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"LightGBM with Best Parameters:\")\n",
    "print(f\"Accuracy: {lgbm_accuracy}\")\n",
    "print(f\"ROC-AUC: {lgbm_roc_auc}\")\n",
    "print(f\"Precision: {lgbm_precision}\")\n",
    "print(f\"Recall: {lgbm_recall}\")\n",
    "print(f\"F1: {lgbm_f1}\")\n",
    "print(f\"Balanced Accuracy: {lgbm_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Stacking model\n",
    "estimators = [\n",
    "    ('rf', best_rf),\n",
    "    ('gb', best_gb),\n",
    "    ('lgbm', best_lgbm)\n",
    "]\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f90a2d5-0dcc-41eb-a627-824d09301131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Random Forest: {'n_estimators': 150, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30, 'bootstrap': False}\n",
      "Best ROC-AUC score for Random Forest: 0.9837353605871594\n",
      "Random Forest with Best Parameters:\n",
      "Accuracy: 0.8864767073722297\n",
      "ROC-AUC: 0.6416505104322692\n",
      "Precision: 0.39669421487603307\n",
      "Recall: 0.021486123545210387\n",
      "F1: 0.04076433121019108\n",
      "Balanced Accuracy: 0.5086768291091464\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Gradient Boosting: {'n_estimators': 94, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 9, 'learning_rate': 0.1}\n",
      "Best ROC-AUC score for Gradient Boosting: 0.9524422195685772\n",
      "Gradient Boosting with Best Parameters:\n",
      "Accuracy: 0.8873812754409769\n",
      "ROC-AUC: 0.6709184993466133\n",
      "Precision: 0.4774193548387097\n",
      "Recall: 0.03312444046553268\n",
      "F1: 0.06195060694851402\n",
      "Balanced Accuracy: 0.5142695511130381\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 70578, number of negative: 70578\n",
      "[LightGBM] [Info] Total Bins 9314\n",
      "[LightGBM] [Info] Number of data points in the train set: 141156, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best parameters for LightGBM: {'subsample': 0.8, 'num_leaves': 127, 'n_estimators': 150, 'min_child_samples': 20, 'max_depth': 20, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "Best ROC-AUC score for LightGBM: 0.9531840448072996\n",
      "LightGBM with Best Parameters:\n",
      "Accuracy: 0.8878335594753505\n",
      "ROC-AUC: 0.6757828034485441\n",
      "Precision: 0.5096153846153846\n",
      "Recall: 0.023724261414503133\n",
      "F1: 0.045337895637296836\n",
      "Balanced Accuracy: 0.5104185982985338\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 70578, number of negative: 70578\n",
      "[LightGBM] [Info] Total Bins 9314\n",
      "[LightGBM] [Info] Number of data points in the train set: 141156, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56463\n",
      "[LightGBM] [Info] Number of positive: 56463, number of negative: 56462\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56463\n",
      "[LightGBM] [Info] Number of positive: 56463, number of negative: 56462\n",
      "[LightGBM] [Info] Number of positive: 56462, number of negative: 56462\n",
      "[LightGBM] [Info] Total Bins 9015\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000018\n",
      "[LightGBM] [Info] Start training from score 0.000018\n",
      "[LightGBM] [Info] Total Bins 9020\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500004 -> initscore=0.000018\n",
      "[LightGBM] [Info] Start training from score 0.000018\n",
      "[LightGBM] [Info] Total Bins 9024\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 72\n",
      "[LightGBM] [Info] Total Bins 9027\n",
      "[LightGBM] [Info] Number of data points in the train set: 112925, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499996 -> initscore=-0.000018\n",
      "[LightGBM] [Info] Start training from score -0.000018\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499996 -> initscore=-0.000018\n",
      "[LightGBM] [Info] Start training from score -0.000018\n",
      "[LightGBM] [Info] Total Bins 9274\n",
      "[LightGBM] [Info] Number of data points in the train set: 112924, number of used features: 73\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Stacking Model:\n",
      "Accuracy: 0.8755716367656666\n",
      "ROC-AUC: 0.6292448663464898\n",
      "Precision: 0.28771929824561404\n",
      "Recall: 0.07341092211280215\n",
      "F1: 0.11697574893009986\n",
      "Balanced Accuracy: 0.525213810900726\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8893\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8952\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 76\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8904\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8999\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 70\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8919\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9036\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8931\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 8922\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47052, number of negative: 47052\n",
      "[LightGBM] [Info] Total Bins 9013\n",
      "[LightGBM] [Info] Number of data points in the train set: 94104, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "from lightgbm import LGBMClassifier\n",
    "import contextlib\n",
    "import os\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('diabetic_data.csv')\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "data.drop(columns=['weight', 'payer_code', 'medical_specialty'], inplace=True)\n",
    "data.dropna(subset=['race', 'gender', 'age'], inplace=True)\n",
    "\n",
    "# Convert 'age' to a numerical average\n",
    "data['age'] = data['age'].apply(lambda x: (int(x.split('-')[0][1:]) + int(x.split('-')[1][:-1])) // 2)\n",
    "\n",
    "# Feature Engineering\n",
    "data['num_medications_age'] = data['num_medications'] * data['age']\n",
    "data['num_lab_procedures_num_medications'] = data['num_lab_procedures'] * data['num_medications']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['race', 'gender', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Encode the target variable\n",
    "data['readmitted'] = data['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "# Drop rows with specific discharge_disposition_id values\n",
    "data = data[~data['discharge_disposition_id_11'].isin([11, 13, 14, 19, 20, 21])]\n",
    "\n",
    "# Define features and target variable\n",
    "X = data.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])\n",
    "y = data['readmitted']\n",
    "\n",
    "# Encode any remaining non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    X[col] = pd.Categorical(X[col]).codes\n",
    "\n",
    "# Split the dataset into training and validation sets with stratified sampling\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Apply SMOTETomek to the training data\n",
    "smotetomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smotetomek.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Feature selection with Lasso\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(X_train_resampled, y_train_resampled)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n",
    "\n",
    "# Use selected features\n",
    "X_train_selected = X_train_resampled[:, importance > 0]\n",
    "X_valid_selected = X_valid_scaled[:, importance > 0]\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X_valid, y_valid, threshold=0.5):\n",
    "    y_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    roc_auc = roc_auc_score(y_valid, y_prob)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    f1 = f1_score(y_valid, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_valid, y_pred)\n",
    "    return accuracy, roc_auc, precision, recall, f1, balanced_accuracy\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [int(x) for x in np.linspace(5, 30, num=6)],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(rf, param_distributions=param_dist_rf, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_rf.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Random Forest model\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", random_search_rf.best_params_)\n",
    "print(\"Best ROC-AUC score for Random Forest:\", random_search_rf.best_score_)\n",
    "\n",
    "# Evaluate Random Forest on the validation set\n",
    "rf_accuracy, rf_roc_auc, rf_precision, rf_recall, rf_f1, rf_balanced_accuracy = evaluate_model(best_rf, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Random Forest with Best Parameters:\")\n",
    "print(f\"Accuracy: {rf_accuracy}\")\n",
    "print(f\"ROC-AUC: {rf_roc_auc}\")\n",
    "print(f\"Precision: {rf_precision}\")\n",
    "print(f\"Recall: {rf_recall}\")\n",
    "print(f\"F1: {rf_f1}\")\n",
    "print(f\"Balanced Accuracy: {rf_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Gradient Boosting model\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "param_dist_gb = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(gb, param_distributions=param_dist_gb, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_gb.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best Gradient Boosting model\n",
    "best_gb = random_search_gb.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", random_search_gb.best_params_)\n",
    "print(\"Best ROC-AUC score for Gradient Boosting:\", random_search_gb.best_score_)\n",
    "\n",
    "# Evaluate Gradient Boosting on the validation set\n",
    "gb_accuracy, gb_roc_auc, gb_precision, gb_recall, gb_f1, gb_balanced_accuracy = evaluate_model(best_gb, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Gradient Boosting with Best Parameters:\")\n",
    "print(f\"Accuracy: {gb_accuracy}\")\n",
    "print(f\"ROC-AUC: {gb_roc_auc}\")\n",
    "print(f\"Precision: {gb_precision}\")\n",
    "print(f\"Recall: {gb_recall}\")\n",
    "print(f\"F1: {gb_f1}\")\n",
    "print(f\"Balanced Accuracy: {gb_balanced_accuracy}\")\n",
    "\n",
    "# Initialize LightGBM model\n",
    "lgbm = LGBMClassifier(random_state=42, force_row_wise=True)\n",
    "\n",
    "# Hyperparameter tuning for LightGBM\n",
    "param_dist_lgbm = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=150, num=10)],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 62, 127],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "random_search_lgbm = RandomizedSearchCV(lgbm, param_distributions=param_dist_lgbm, n_iter=50, cv=3, scoring='roc_auc', n_jobs=-1, random_state=42, verbose=1, error_score='raise')\n",
    "random_search_lgbm.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Best LightGBM model\n",
    "best_lgbm = random_search_lgbm.best_estimator_\n",
    "print(\"Best parameters for LightGBM:\", random_search_lgbm.best_params_)\n",
    "print(\"Best ROC-AUC score for LightGBM:\", random_search_lgbm.best_score_)\n",
    "\n",
    "# Evaluate LightGBM on the validation set\n",
    "lgbm_accuracy, lgbm_roc_auc, lgbm_precision, lgbm_recall, lgbm_f1, lgbm_balanced_accuracy = evaluate_model(best_lgbm, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"LightGBM with Best Parameters:\")\n",
    "print(f\"Accuracy: {lgbm_accuracy}\")\n",
    "print(f\"ROC-AUC: {lgbm_roc_auc}\")\n",
    "print(f\"Precision: {lgbm_precision}\")\n",
    "print(f\"Recall: {lgbm_recall}\")\n",
    "print(f\"F1: {lgbm_f1}\")\n",
    "print(f\"Balanced Accuracy: {lgbm_balanced_accuracy}\")\n",
    "\n",
    "# Initialize Stacking model\n",
    "estimators = [\n",
    "    ('rf', best_rf),\n",
    "    ('gb', best_gb),\n",
    "    ('lgbm', best_lgbm)\n",
    "]\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the stacking model\n",
    "with contextlib.redirect_stdout(open(os.devnull, 'w')):\n",
    "    stacking_model.fit(X_train_selected, y_train_resampled)\n",
    "\n",
    "# Evaluate Stacking Model on the validation set\n",
    "stacking_accuracy, stacking_roc_auc, stacking_precision, stacking_recall, stacking_f1, stacking_balanced_accuracy = evaluate_model(stacking_model, X_valid_selected, y_valid)\n",
    "\n",
    "print(\"Stacking Model:\")\n",
    "print(f\"Accuracy: {stacking_accuracy}\")\n",
    "print(f\"ROC-AUC: {stacking_roc_auc}\")\n",
    "print(f\"Precision: {stacking_precision}\")\n",
    "print(f\"Recall: {stacking_recall}\")\n",
    "print(f\"F1: {stacking_f1}\")\n",
    "print(f\"Balanced Accuracy: {stacking_balanced_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1089c4-365c-48f1-9875-14d9312fb7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
